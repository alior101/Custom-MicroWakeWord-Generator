{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJm2obb5TJ2h"
   },
   "source": [
    "**Introduction**:\n",
    "\n",
    "This notebook builds a dataset inside colab and potentially saves it in google drive for later usage.\n",
    "\n",
    "Typically, instead of regenrating from scratch a dataset, one would want to augment it and retrain the model. The workflow would therefore be to set the \"restore_dataset\" variable accordingly.\n",
    "\n",
    "If restore is set to false, all samples are re-generated, otherwhise, they are reloaded and the user can augement with voice samples or any other samples. It's recommended to save the dataset back into the drive after augmenting for later usage (notebook 02-features-generation for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab - will use gdrive utility to backup/restore datasets and features. Please configure gdrive to access your gdrive..\n"
     ]
    }
   ],
   "source": [
    "# Install all the required packages (borrowed from openWakeWord's automatic training notebook)\n",
    "running_on_colab = False \n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Running on CoLab')\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    running_on_colab = True\n",
    "    restore_dataset = True\n",
    "    restore_features = True\n",
    "else:\n",
    "    print('Not running on CoLab - will use gdrive utility to backup/restore datasets and features. Please configure gdrive to access your gdrive..')\n",
    "    restore_dataset = False\n",
    "    restore_features = False\n",
    " \n",
    "\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if restore_dataset:\n",
    "  dataset_filename = \"/content/drive/MyDrive/ColabNotebooks/VoiceAssistant/microWakeWord/datasets_20240423_175513.tar\"\n",
    "  !cp {dataset_filename}  .\n",
    "  dataset_file = os.path.basename(dataset_filename)\n",
    "  !tar -xvf ./{dataset_file} to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torch\n",
    "%pip install -q torchaudio\n",
    "%pip install -q scipy\n",
    "%pip install -q tqdm\n",
    "%pip install -q jupyter\n",
    "%pip install -q ipywidgets\n",
    "%pip install -q mutagen\n",
    "%pip install -q torchinfo\n",
    "%pip install -q torchmetrics\n",
    "%pip install -q speechbrain\n",
    "%pip install -q audiomentations\n",
    "%pip install -q torch-audiomentations\n",
    "%pip install -q acoustics\n",
    "%pip install -q pronouncing\n",
    "%pip install -q datasets\n",
    "%pip install -q deep-phonemizer\n",
    "%pip install -q piper-phonemize\n",
    "%pip install -q webrtcvad\n",
    "%pip install -q datasets\n",
    "%pip install -q mmap_ninja\n",
    "%pip install -q gradio\n",
    "%pip install -q tensorflow==2.15.0\n",
    "%pip install -q keras==2.15.0\n",
    "%pip install -q sounddevice\n",
    "%pip install -q tflite_micro\n",
    "#%pip install cloud-tpu-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if not os.path.isdir(\"./piper-sample-generator\"):\n",
    "    !git clone -b mps-support https://github.com/kahrendt/piper-sample-generator\n",
    "    !wget -O piper-sample-generator/models/en_US-libritts_r-medium.pt 'https://github.com/rhasspy/piper-sample-generator/releases/download/v2.0.0/en_US-libritts_r-medium.pt'\n",
    "\n",
    "\n",
    "if not os.path.isdir(\"./openWakeWord\"):\n",
    "    !git clone https://github.com/dscripka/openWakeWord\n",
    "\n",
    "cwd = os.getcwd()\n",
    "piper_path = cwd+\"/piper-sample-generator/\"\n",
    "if piper_path not in sys.path:\n",
    "    sys.path.insert(0, piper_path)\n",
    "\n",
    "oww_path = cwd + \"/openWakeWord/openwakeword/\"\n",
    "if  oww_path not in sys.path:\n",
    "    sys.path.insert(0, oww_path)\n",
    "\n",
    "print(\"Updated sys.path is \")\n",
    "print(\"\\n\".join(sys.path))\n",
    "print(\"Please restart the kernel (Kernel -> Restart Kernel) and run the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from mmap_ninja.ragged import RaggedMmap\n",
    "import os\n",
    "import uuid\n",
    "import logging\n",
    "import datasets\n",
    "import scipy\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import yaml\n",
    "import logging\n",
    "from generate_samples import generate_samples\n",
    "from data import generate_adversarial_texts\n",
    "\n",
    "!mkdir -p datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download noise and background audio (takes about ~3 minutes)\n",
    "\n",
    "# Audioset Dataset (https://research.google.com/audioset/dataset/index.html)\n",
    "# Download one part of the audioset .tar files, extract, and convert to 16khz\n",
    "# For full-scale training, it's recommended to download the entire dataset from\n",
    "# https://huggingface.co/datasets/agkphysics/AudioSet, and\n",
    "# even potentially combine it with other background noise datasets (e.g., FSD50k, Freesound, etc.)\n",
    "from tqdm import tqdm\n",
    "if not os.path.exists(\"datasets/audioset\"):\n",
    "    os.mkdir(\"audioset\")\n",
    "\n",
    "    fname = \"bal_train09.tar\"\n",
    "    out_dir = f\"datasets/audioset/{fname}\"\n",
    "    link = \"https://huggingface.co/datasets/agkphysics/AudioSet/resolve/main/data/\" + fname\n",
    "    !wget -O {out_dir} {link}\n",
    "    !cd datasets/audioset && tar -xvf bal_train09.tar\n",
    "\n",
    "# convert them to wav at 16k sampling rate\n",
    "output_dir = \"./datasets/audioset_16k\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "    # Save clips to 16-bit PCM wav files\n",
    "    audioset_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(\"audioset/audio\").glob(\"**/*.flac\")]})\n",
    "    audioset_dataset = audioset_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
    "    for row in tqdm(audioset_dataset):\n",
    "        name = row['audio']['path'].split('/')[-1].replace(\".flac\", \".wav\")\n",
    "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./alexa_phonemes.txt\n",
    "ə lɛk sə,\n",
    "ə lɛk sa, \n",
    "a lɛk sa, \n",
    "ʌ lɛk sa,\n",
    "ʌ lɛk sʌ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_pronounciation_dataset = 'datasets/positive_more_pronunciations/alexa/'\n",
    "for d in ['positive_train', 'positive_test', 'positive_validation']:\n",
    "    if not os.path.exists(more_pronounciation_dataset + d):\n",
    "        os.makedirs(more_pronounciation_dataset + d)\n",
    "\n",
    "more_pronounciation_samples = {}\n",
    "more_pronounciation_samples['positive_train'] = 100000\n",
    "more_pronounciation_samples['positive_test'] = 1000\n",
    "more_pronounciation_samples['positive_validation'] = 1000\n",
    "\n",
    "for d in ['positive_train', 'positive_test', 'positive_validation']:\n",
    "    n_current_samples = len(os.listdir(more_pronounciation_dataset + d))\n",
    "    print(\"Currently there are \", n_current_samples, \" files in \", d )\n",
    "    if n_current_samples <= 0.95*more_pronounciation_samples[d]:\n",
    "        !python3 piper-sample-generator/generate_samples.py alexa_phonemes.txt \\\n",
    "            --max-samples {more_pronounciation_samples[d]} \\\n",
    "            --batch-size 10 \\\n",
    "            --slerp-weights 1 \\\n",
    "            --phoneme-input \\\n",
    "            --output-dir {more_pronounciation_dataset + d} \\\n",
    "            --max-speakers 600 \\\n",
    "            --min-phoneme-count 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Free Music Archive dataset\n",
    "# https://github.com/mdeff/fma\n",
    "\n",
    "output_dir = \"./datasets/fma\"\n",
    "if not os.path.exists(output_dir):\n",
    "  os.mkdir(output_dir)\n",
    "  fma_dataset = datasets.load_dataset(\"rudraml/fma\", name=\"small\", split=\"train\", streaming=False)\n",
    "  fma_dataset = iter(fma_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000)))\n",
    "  # Save clips to 16-bit PCM wav files\n",
    "  n_hours = 1  # use only 1 hour of clips for this example notebook, recommend increasing for full-scale training\n",
    "  for i in tqdm(range(n_hours*3600//30)):  # this works because the FMA dataset is all 30 second clips\n",
    "      row = next(fma_dataset)\n",
    "      name = row['audio']['path'].split('/')[-1].replace(\".mp3\", \".wav\")\n",
    "      scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
    "      i += 1\n",
    "      if i == n_hours*3600//30:\n",
    "          break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the wake word with phonetic transcription\n",
    "config = {}\n",
    "config[\"target_phrase\"] = 'alexha'\n",
    "config[\"target_phrase_as_written\"] = 'alexha'       # used to generate adverserial phrases\n",
    "config[\"model_name\"] = 'alexa'\n",
    "config[\"custom_negative_phrases\"] = ['ali', 'aliba', 'yooba', 'liba', 'tessa','alex','exa', 'ale']\n",
    "\n",
    "config[\"samples_output_dir\"] = 'datasets/generated_samples'\n",
    "config[\"features_output_dir\"] = 'training_features'\n",
    "config[\"rirs_dir\"] = 'datasets/BUT_ReverbDB_rel_19_06_RIR-Only'  # directory containing Room Imnpulse Response files\n",
    "config[\"rirs_glob\"] = \"**/RIR/*.wav\"                    # Glob to choose the appropriate wav files for RIR\n",
    "config[\"audioset_clips_dir\"] = 'datasets/audioset_16k'           # directory containing converted Audioset wav files\n",
    "config[\"fma_clips_dir\"] = 'datasets/fma'                         # directory containing converted FreeMusicArchive wav files\n",
    "config[\"fsd_clips_dir\"] = 'datasets/fsd'                         # directory containing converted FSD50K wav files\n",
    "\n",
    "config[\"n_samples\"] = 250000 #200000                            # number of training samples to generate\n",
    "config[\"n_samples_val\"] =  20000 # 20000                         # number of testing and validation samples to generate\n",
    "config[\"tts_batch_size\"] = 20\n",
    "config[\"augment_batch_size\"] = 16\n",
    "config[\"clip_duration_ms\"] = 1430                       # generated clips longer than this are ignored when augmenting\n",
    "config[\"spectrogram_duration_ms\"] = 1490                # duration of the spectrogram (usually equivalent to clip_duration_ms + end_jitter_ms)\n",
    "config[\"sample_rate_hz\"] = 16000\n",
    "config[\"end_jitter_ms\"] = 60                            # augmented clips have up to this amount of blank noise at the end of the clip\n",
    "\n",
    "\n",
    "config[\"samples_output_dir\"] = os.path.abspath(config[\"samples_output_dir\"])\n",
    "\n",
    "if not os.path.exists(config[\"samples_output_dir\"]):\n",
    "    os.mkdir(config[\"samples_output_dir\"])\n",
    "\n",
    "models_samples_directory = os.path.join(config[\"samples_output_dir\"], config[\"model_name\"])\n",
    "if not os.path.exists(models_samples_directory):\n",
    "    os.mkdir(models_samples_directory)\n",
    "\n",
    "positive_train_output_dir = os.path.join(models_samples_directory, \"positive_train\")\n",
    "positive_test_output_dir = os.path.join(models_samples_directory, \"positive_test\")\n",
    "positive_validation_output_dir = os.path.join(models_samples_directory, \"positive_validation\")\n",
    "negative_train_output_dir = os.path.join(models_samples_directory, \"negative_train\")\n",
    "negative_test_output_dir = os.path.join(models_samples_directory, \"negative_test\")\n",
    "negative_validation_output_dir = os.path.join(models_samples_directory, \"negative_validation\")\n",
    "\n",
    "# save the dataset and features generation configuration\n",
    "import json\n",
    "config_str = json.dumps(config)\n",
    "with open('dataset_config.json', 'w') as f:\n",
    "    json.dump(config, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"\\n\" +\"#\"*50 + \"\\nGenerating positive clips for training\\n\" + \"#\"*50)\n",
    "\n",
    "# Generate positive clips for training\n",
    "if not os.path.exists(positive_train_output_dir):\n",
    "    os.mkdir(positive_train_output_dir)\n",
    "\n",
    "n_current_samples = len(os.listdir(positive_train_output_dir))\n",
    "print(\"Currently there are \", n_current_samples, \" files\")\n",
    "if n_current_samples <= 0.95*config[\"n_samples\"]:\n",
    "    generate_samples(\n",
    "        text=config[\"target_phrase\"], max_samples=config[\"n_samples\"]-n_current_samples,\n",
    "        batch_size=config[\"tts_batch_size\"],\n",
    "        noise_scales=[0.98], noise_scale_ws=[0.98], length_scales=[0.75, 1.0, 1.25],\n",
    "        output_dir=positive_train_output_dir, auto_reduce_batch_size=True,\n",
    "        file_names=[uuid.uuid4().hex + \".wav\" for i in range(config[\"n_samples\"])]\n",
    "    )\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    logging.warning(f\"Skipping generation of positive clips for training, as ~{config['n_samples']} already exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate positive clips for testing\n",
    "logging.info(\"\\n\" +\"#\"*50 + \"\\nGenerating positive clips for testing\\n\" + \"#\"*50)\n",
    "if not os.path.exists(positive_test_output_dir):\n",
    "    os.mkdir(positive_test_output_dir)\n",
    "n_current_samples = len(os.listdir(positive_test_output_dir))\n",
    "\n",
    "if n_current_samples <= 0.95*config[\"n_samples_val\"]:\n",
    "    generate_samples(text=config[\"target_phrase\"], max_samples=config[\"n_samples_val\"]-n_current_samples,\n",
    "                     batch_size=config[\"tts_batch_size\"],\n",
    "                     noise_scales=[1.0], noise_scale_ws=[1.0], length_scales=[0.75, 1.0, 1.25],\n",
    "                     output_dir=positive_test_output_dir, auto_reduce_batch_size=True,\n",
    "                     file_names=[uuid.uuid4().hex + \".wav\" for i in range(config[\"n_samples\"])]\n",
    "    )\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    logging.warning(f\"Skipping generation of positive clips testing, as ~{config['n_samples_val']} already exist\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate positive clips for validation\n",
    "logging.info(\"\\n\" + \"#\"*50 + \"\\nGenerating positive clips for validation\\n\" + \"#\"*50)\n",
    "if not os.path.exists(positive_validation_output_dir):\n",
    "    os.mkdir(positive_validation_output_dir)\n",
    "n_current_samples = len(os.listdir(positive_validation_output_dir))\n",
    "print(\"Currently there are \", n_current_samples, \" files\")\n",
    "if n_current_samples <= 0.95*config[\"n_samples_val\"]:\n",
    "    generate_samples(text=config[\"target_phrase\"], max_samples=config[\"n_samples_val\"]-n_current_samples,\n",
    "                     batch_size=config[\"tts_batch_size\"],\n",
    "                     noise_scales=[1.0], noise_scale_ws=[1.0], length_scales=[0.75, 1.0, 1.25],\n",
    "                     output_dir=positive_validation_output_dir, auto_reduce_batch_size=True,\n",
    "                     file_names=[uuid.uuid4().hex + \".wav\" for i in range(config[\"n_samples\"])]\n",
    "    )\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    logging.warning(f\"Skipping generation of positive clips validation, as ~{config['n_samples_val']} already exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate adversarial negative clips for training\n",
    "logging.info(\"\\n\" +\"#\"*50 + \"\\nGenerating negative clips for training\\n\" + \"#\"*50)\n",
    "if not os.path.exists(negative_train_output_dir):\n",
    "    os.mkdir(negative_train_output_dir)\n",
    "n_current_samples = len(os.listdir(negative_train_output_dir))\n",
    "while n_current_samples <= 0.95*config[\"n_samples\"]:\n",
    "    print(\"Currently there are \", n_current_samples, \" files\")\n",
    "    adversarial_texts = config[\"custom_negative_phrases\"]\n",
    "    for target_phrase in config[\"target_phrase\"]:\n",
    "        adversarial_texts.extend(generate_adversarial_texts(\n",
    "            input_text=target_phrase,\n",
    "            N=config[\"n_samples\"]//len(config[\"target_phrase\"]),\n",
    "            include_partial_phrase=1.0,\n",
    "            include_input_words=0.2))\n",
    "    generate_samples(text=adversarial_texts, max_samples=config[\"n_samples\"]-n_current_samples,\n",
    "                     batch_size=config[\"tts_batch_size\"]//7,\n",
    "                     noise_scales=[0.98], noise_scale_ws=[0.98], length_scales=[0.75, 1.0, 1.25],\n",
    "                     output_dir=negative_train_output_dir, auto_reduce_batch_size=True,\n",
    "                     file_names=[uuid.uuid4().hex + \".wav\" for i in range(config[\"n_samples\"])]\n",
    "                     )\n",
    "    n_current_samples = len(os.listdir(negative_train_output_dir))\n",
    "    print(\"Currently there are \", n_current_samples, \" files\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    logging.warning(f\"Skipping generation of negative clips for training, as ~{config['n_samples']} already exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate adversarial negative clips for testing\n",
    "logging.info(\"\\n\" +\"#\"*50 + \"\\nGenerating negative clips for testing\\n\" + \"#\"*50)\n",
    "if not os.path.exists(negative_test_output_dir):\n",
    "    os.mkdir(negative_test_output_dir)\n",
    "n_current_samples = len(os.listdir(negative_test_output_dir))\n",
    "print(\"Currently there are \", n_current_samples, \" files - requiring \", config[\"n_samples_val\"], \" files.\")\n",
    "if n_current_samples <= 0.95*config[\"n_samples_val\"]:\n",
    "    adversarial_texts = config[\"custom_negative_phrases\"]\n",
    "    for target_phrase in config[\"target_phrase\"]:\n",
    "        adversarial_texts.extend(generate_adversarial_texts(\n",
    "            input_text=target_phrase,\n",
    "            N=config[\"n_samples_val\"]//len(config[\"target_phrase\"]),\n",
    "            include_partial_phrase=1.0,\n",
    "            include_input_words=0.2))\n",
    "    generate_samples(text=adversarial_texts, max_samples=config[\"n_samples_val\"]-n_current_samples,\n",
    "                     batch_size=config[\"tts_batch_size\"]//7,\n",
    "                     noise_scales=[1.0], noise_scale_ws=[1.0], length_scales=[0.75, 1.0, 1.25],\n",
    "                     output_dir=negative_test_output_dir, auto_reduce_batch_size=True,\n",
    "                     file_names=[uuid.uuid4().hex + \".wav\" for i in range(config[\"n_samples\"])]\n",
    "    )\n",
    "    n_current_samples = len(os.listdir(negative_test_output_dir))\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    logging.warning(f\"Skipping generation of negative clips for testing, as ~{config['n_samples_val']} already exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate adversarial negative clips for validation\n",
    "logging.info(\"#\"*50 + \"\\nGenerating negative clips for validation\\n\" + \"#\"*50)\n",
    "if not os.path.exists(negative_validation_output_dir):\n",
    "    os.mkdir(negative_validation_output_dir)\n",
    "n_current_samples = len(os.listdir(negative_validation_output_dir))\n",
    "print(\"Currently there are \", n_current_samples, \" files\")\n",
    "if n_current_samples <= 0.95*config[\"n_samples_val\"]:\n",
    "    adversarial_texts = config[\"custom_negative_phrases\"]\n",
    "    for target_phrase in config[\"target_phrase\"]:\n",
    "        adversarial_texts.extend(generate_adversarial_texts(\n",
    "            input_text=target_phrase,\n",
    "            N=config[\"n_samples_val\"]//len(config[\"target_phrase\"]),\n",
    "            include_partial_phrase=1.0,\n",
    "            include_input_words=0.2))\n",
    "    generate_samples(text=adversarial_texts, max_samples=config[\"n_samples_val\"]-n_current_samples,\n",
    "                     batch_size=config[\"tts_batch_size\"]//7,\n",
    "                     noise_scales=[1.0], noise_scale_ws=[1.0], length_scales=[0.75, 1.0, 1.25],\n",
    "                     output_dir=negative_validation_output_dir, auto_reduce_batch_size=True,\n",
    "                     file_names=[uuid.uuid4().hex + \".wav\" for i in range(config[\"n_samples\"])]\n",
    "    )\n",
    "    n_current_samples = len(os.listdir(negative_validation_output_dir))\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    logging.warning(f\"Skipping generation of negative clips for validation, as ~{config['n_samples_val']} already exist\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment generated samples by adding background noise and applying room impulse responses\n",
    "from data import augment_clips, filter_audio_paths\n",
    "\n",
    "config[\"rirs_dir\"] = os.path.abspath(config[\"rirs_dir\"])\n",
    "config[\"audioset_clips_dir\"] = os.path.abspath(config[\"audioset_clips_dir\"])\n",
    "config[\"fma_clips_dir\"] = os.path.abspath(config[\"fma_clips_dir\"])\n",
    "config[\"fsd_clips_dir\"] = os.path.abspath(config[\"fsd_clips_dir\"])\n",
    "config[\"features_output_dir\"] = os.path.abspath(config[\"features_output_dir\"])\n",
    "config[\"audio_samples_per_clip\"] = int((config[\"spectrogram_duration_ms\"])*config[\"sample_rate_hz\"]/1000) # ms * herz *1/(1000ms) = # of samples\n",
    "\n",
    "if not os.path.exists(config[\"features_output_dir\"]):\n",
    "    os.mkdir(config[\"features_output_dir\"])\n",
    "\n",
    "max_duration_sec = config[\"clip_duration_ms\"]/1000.0\n",
    "spectrogram_duration_sec = config[\"spectrogram_duration_ms\"]/1000.0\n",
    "jitter_s = config[\"end_jitter_ms\"]/1000.0\n",
    "\n",
    "positive_clips_train, durations = filter_audio_paths([positive_train_output_dir],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"size\")\n",
    "positive_clips_test, durations = filter_audio_paths([positive_test_output_dir],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"size\")\n",
    "positive_clips_validation, durations = filter_audio_paths([positive_validation_output_dir],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"size\")\n",
    "\n",
    "negative_clips_train, durations = filter_audio_paths([negative_train_output_dir],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"size\")\n",
    "negative_clips_test, durations = filter_audio_paths([negative_test_output_dir],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"size\")\n",
    "negative_clips_validation, durations = filter_audio_paths([negative_validation_output_dir],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"size\")\n",
    "\n",
    "rir_paths = [str(i) for i in Path(config[\"rirs_dir\"]).glob(config[\"rirs_glob\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download noise and silence files and put in datasets\n",
    "# all non relevant files in \"silence\" dirs can be removed with\n",
    "# find . -type d -name \"silence\" -exec rm -rf {} +\n",
    "if not os.path.exists(\"datasets/BUT_ReverbDB_rel_19_06_RIR-Only\"):\n",
    "  if os.path.isfile(\"./BUT_ReverbDB_rel_19_06_RIR-Only.tgz\") == False:\n",
    "    !wget http://merlin.fit.vutbr.cz/ReverbDB/BUT_ReverbDB_rel_19_06_RIR-Only.tgz\n",
    "  !tar -xvf ./BUT_ReverbDB_rel_19_06_RIR-Only.tgz ./datasets/BUT_ReverbDB_rel_19_06_RIR-Only/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q TTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diyYap04xkLw"
   },
   "source": [
    "Trials with coquiTTS for adapting general voices to user voice profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/home/lior/anaconda3/envs/google_kws/bin/python\n",
    "import torch\n",
    "from TTS.api import TTS\n",
    "tts = TTS(model_name=\"voice_conversion_models/multilingual/vctk/freevc24\", progress_bar=False).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "from scipy.io import wavfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "if not os.path.exists(\"./datasets/cloned_samples\"):\n",
    "  os.mkdir(\"./datasets/cloned_samples\")\n",
    "\n",
    "for target_dir in [\"positive_train\", \"positive_test\",\n",
    "                   \"positive_validation\", \"negative_train\", \"negative_test\", \"negative_validation\"]:\n",
    "\n",
    "  print(f'processing {target_dir}')\n",
    "  if not os.path.exists(f'./datasets/cloned_samples/{target_dir}'):\n",
    "      os.mkdir(f'./datasets/cloned_samples/{target_dir}')\n",
    "\n",
    "  dirA = f'./datasets/generated_samples/alexa/{target_dir}/'\n",
    "  target_voice = \"./datasets/captured_samples/positive_train/positive0da5fca5153246a8ae2e631f666622c8.wav\"\n",
    "  dirB = f'./datasets/cloned_samples/{target_dir}/'\n",
    "\n",
    "  # Get a list of all wav files in dirA\n",
    "  wav_files = glob.glob(os.path.join(dirA, \"*.wav\"))\n",
    "\n",
    "  for wav_file in tqdm(wav_files[1:2000], desc=\"Processing items\", unit=\"item\"):\n",
    "      filename = os.path.basename(wav_file)\n",
    "      target_file = os.path.join(dirB, filename)\n",
    "\n",
    "      # Check if the file exists in dirB\n",
    "      if os.path.exists(target_file):\n",
    "          #print(f\"File {filename} already exists in {dirB}, skipping.\")\n",
    "          continue\n",
    "\n",
    "      # If not, apply a function on it and save it in dirB\n",
    "      tts.voice_conversion_to_file(source_wav=wav_file, target_wav=target_voice, file_path=target_file)\n",
    "      #print(f\"Processed and saved file {filename} in {dirB}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# coqui is generating 24Khz sample rate - covert to 16Khz\n",
    "import os\n",
    "import librosa\n",
    "from scipy.io.wavfile import write\n",
    "import soundfile as sf\n",
    "\n",
    "def resample_wav_file(filepath, target_sr):\n",
    "    # Load the audio file with original sample rate\n",
    "    y, sr = librosa.load(filepath, sr=None)\n",
    "\n",
    "    # Resample to target sample rate\n",
    "    y_resampled = librosa.resample(y, orig_sr=sr, target_sr=target_sr)\n",
    "\n",
    "    # Save the resampled audio\n",
    "    sf.write(filepath, y_resampled, target_sr, subtype='PCM_16')\n",
    "    #write(filepath, target_sr, y_resampled)\n",
    "\n",
    "# Define the directory\n",
    "directory = \"./datasets/cloned_samples\"\n",
    "\n",
    "# Define the target sample rate\n",
    "target_sr = 16000\n",
    "\n",
    "# Walk through the directory\n",
    "for dirpath, dirnames, filenames in os.walk(directory):\n",
    "    for filename in tqdm(filenames):\n",
    "        if filename.endswith('.wav'):\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            resample_wav_file(filepath, target_sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.1GiB 0:11:04 [32.5MiB/s] [================================>] 102%            \n",
      "Uploading ./datasets_20240428_133701.tar\n",
      "File successfully uploaded\n",
      "Id: 1xSVLo9abJYq7_BEmlyQc9G8rPSvrQdy9\n",
      "Name: datasets_20240428_133701.tar\n",
      "Mime: application/x-tar\n",
      "Size: 22.7 GB\n",
      "Created: 2024-04-28 13:58:22\n",
      "Modified: 2024-04-28 13:58:22\n",
      "MD5: 6049bc30edd1a61688f2d3e78a2154ff\n",
      "Shared: True\n",
      "Parents: 1xLbJBpoHUCWDELtzMggekID3ZkSMaU7-\n",
      "ViewUrl: https://drive.google.com/file/d/1xSVLo9abJYq7_BEmlyQc9G8rPSvrQdy9/view?usp=drivesdk\n"
     ]
    }
   ],
   "source": [
    "if running_on_colab == True:\n",
    "    !tar -cvf ./datasets_$(date +%Y%m%d_%H%M%S).tar ./datasets\n",
    "    !cp ./datasets*.tar  /content/drive/MyDrive/ColabNotebooks/VoiceAssistant/microWakeWord/\n",
    "else:\n",
    "    #!tar -cf ./datasets_$(date +%Y%m%d_%H%M%S).tar ./datasets\n",
    "    !tar cf - ./datasets | pv -s $(du -sb ./datasets | awk '{print $1}')  > ./datasets_$(date +%Y%m%d_%H%M%S).tar \n",
    "    # TODO: Replace with the dir ID of your gdrive dataset\n",
    "    !gdrive files upload ./datasets*.tar --parent \"1xLbJBpoHUCWDELtzMggekID3ZkSMaU7-\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "google_kws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
