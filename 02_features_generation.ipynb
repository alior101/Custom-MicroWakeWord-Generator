{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab\n"
     ]
    }
   ],
   "source": [
    "# Install all the required packages (borrowed from openWakeWord's automatic training notebook)\n",
    "running_on_colab = False\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  print('Running on CoLab')\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  running_on_colab = True\n",
    "else:\n",
    "  print('Not running on CoLab')\n",
    "  restore_dataset = False\n",
    "  restore_features = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if running_on_colab:\n",
    "  dataset_filename = \"datasets_20240423_175513.tar\"\n",
    "  features_filename = \"training_features_20240421_155624.tar\"\n",
    "  if restore_dataset:\n",
    "    !cp /content/drive/MyDrive/ColabNotebooks/VoiceAssistant/microWakeWord/{dataset_filename}  .\n",
    "    !tar -xvf ./{dataset_filename}\n",
    "  if restore_features:\n",
    "    !cp /content/drive/MyDrive/ColabNotebooks/VoiceAssistant/microWakeWord/{features_filename}  .\n",
    "    !tar -xvf ./{features_filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torch\n",
    "%pip install -q torchaudio\n",
    "%pip install -q scipy\n",
    "%pip install -q tqdm\n",
    "%pip install -q jupyter\n",
    "%pip install -q ipywidgets\n",
    "%pip install -q mutagen\n",
    "%pip install -q torchinfo\n",
    "%pip install -q torchmetrics\n",
    "%pip install -q speechbrain\n",
    "%pip install -q audiomentations\n",
    "%pip install -q torch-audiomentations\n",
    "%pip install -q acoustics\n",
    "%pip install -q pronouncing\n",
    "%pip install -q datasets\n",
    "%pip install -q deep-phonemizer\n",
    "%pip install -q piper-phonemize\n",
    "%pip install -q webrtcvad\n",
    "%pip install -q datasets\n",
    "%pip install -q mmap_ninja\n",
    "%pip install -q gradio\n",
    "%pip install -q tensorflow==2.15.0\n",
    "%pip install -q keras==2.15.0\n",
    "%pip install -q sounddevice\n",
    "%pip install -q tflite_micro\n",
    "#%pip install cloud-tpu-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated sys.path is \n",
      "/home/lior/Custom-MicroWakeWord-Generator/openWakeWord/openwakeword/\n",
      "/home/lior/Custom-MicroWakeWord-Generator\n",
      "/opt/conda/envs/google_kws/lib/python311.zip\n",
      "/opt/conda/envs/google_kws/lib/python3.11\n",
      "/opt/conda/envs/google_kws/lib/python3.11/lib-dynload\n",
      "\n",
      "/opt/conda/envs/google_kws/lib/python3.11/site-packages\n",
      "Please restart the kernel (Kernel -> Restart Kernel) and run the next cell.\n"
     ]
    }
   ],
   "source": [
    "# Dataset generation\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# if not os.path.isdir(\"./piper-sample-generator\"):\n",
    "#     !git clone https://github.com/rhasspy/piper-sample-generator\n",
    "#     !wget -O piper-sample-generator/models/en_US-libritts_r-medium.pt 'https://github.com/rhasspy/piper-sample-generator/releases/download/v2.0.0/en_US-libritts_r-medium.pt'\n",
    "\n",
    "\n",
    "if not os.path.isdir(\"./openWakeWord\"):\n",
    "    !git clone https://github.com/dscripka/openWakeWord\n",
    "\n",
    "cwd = os.getcwd()\n",
    "# piper_path = cwd+\"/piper-sample-generator/\"\n",
    "# if piper_path not in sys.path:\n",
    "#     sys.path.insert(0, piper_path)\n",
    "\n",
    "oww_path = cwd + \"/openWakeWord/openwakeword/\"\n",
    "if  oww_path not in sys.path:\n",
    "    sys.path.insert(0, oww_path)\n",
    "\n",
    "print(\"Updated sys.path is \")\n",
    "print(\"\\n\".join(sys.path))\n",
    "print(\"Please restart the kernel (Kernel -> Restart Kernel) and run the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchvision is not available - cannot save figures\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from mmap_ninja.ragged import RaggedMmap\n",
    "import os\n",
    "import uuid\n",
    "import logging\n",
    "import datasets\n",
    "import scipy\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import yaml\n",
    "import logging\n",
    "# from generate_samples import generate_samples\n",
    "from data import generate_adversarial_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target_phrase': 'alexha', 'target_phrase_as_written': 'alexha', 'model_name': 'alexa', 'custom_negative_phrases': ['ali', 'aliba', 'yooba', 'liba', 'tessa', 'alex', 'exa', 'ale'], 'samples_output_dir': '/home/lior/Custom-MicroWakeWord-Generator/datasets/generated_samples', 'features_output_dir': 'training_features', 'rirs_dir': 'datasets/BUT_ReverbDB_rel_19_06_RIR-Only', 'rirs_glob': '**/RIR/*.wav', 'audioset_clips_dir': 'datasets/audioset_16k', 'fma_clips_dir': 'datasets/fma', 'fsd_clips_dir': 'datasets/fsd', 'n_samples': 250000, 'n_samples_val': 20000, 'tts_batch_size': 20, 'augment_batch_size': 16, 'clip_duration_ms': 1430, 'spectrogram_duration_ms': 1490, 'sample_rate_hz': 16000, 'end_jitter_ms': 60}\n"
     ]
    }
   ],
   "source": [
    "# realod the json config saved in teh dataset geenration notebook\n",
    "import json\n",
    "# Open the file and load the JSON\n",
    "with open('dataset_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Now config is a dictionary with the same content as when it was saved\n",
    "print(config)\n",
    "\n",
    "config[\"samples_output_dir\"] = os.path.abspath(config[\"samples_output_dir\"])\n",
    "\n",
    "if not os.path.exists(config[\"samples_output_dir\"]):\n",
    "    os.mkdir(config[\"samples_output_dir\"])\n",
    "\n",
    "models_samples_directory = os.path.join(config[\"samples_output_dir\"], config[\"model_name\"])\n",
    "if not os.path.exists(models_samples_directory):\n",
    "    os.mkdir(models_samples_directory)\n",
    "\n",
    "positive_train_output_dir = os.path.join(models_samples_directory, \"positive_train\")\n",
    "positive_test_output_dir = os.path.join(models_samples_directory, \"positive_test\")\n",
    "positive_validation_output_dir = os.path.join(models_samples_directory, \"positive_validation\")\n",
    "negative_train_output_dir = os.path.join(models_samples_directory, \"negative_train\")\n",
    "negative_test_output_dir = os.path.join(models_samples_directory, \"negative_test\")\n",
    "negative_validation_output_dir = os.path.join(models_samples_directory, \"negative_validation\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-29 12:54:29--  https://github.com/tensorflow/tflite-micro/raw/main/tensorflow/lite/micro/examples/micro_speech/models/audio_preprocessor_int8.tflite\n",
      "Resolving github.com (github.com)... 140.82.112.3\n",
      "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/tensorflow/tflite-micro/main/tensorflow/lite/micro/examples/micro_speech/models/audio_preprocessor_int8.tflite [following]\n",
      "--2024-04-29 12:54:29--  https://raw.githubusercontent.com/tensorflow/tflite-micro/main/tensorflow/lite/micro/examples/micro_speech/models/audio_preprocessor_int8.tflite\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8772 (8.6K) [application/octet-stream]\n",
      "Saving to: 'audio_preprocessor_int8.tflite'\n",
      "\n",
      "audio_preprocessor_ 100%[===================>]   8.57K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2024-04-29 12:54:32 (8.74 MB/s) - 'audio_preprocessor_int8.tflite' saved [8772/8772]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the preprocessor tf lite mode\n",
    "if os.path.exists('audio_preprocessor_int8.tflite') == False:\n",
    "    !wget https://github.com/tensorflow/tflite-micro/raw/main/tensorflow/lite/micro/examples/micro_speech/models/audio_preprocessor_int8.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_samples_per_clip:  23840\n"
     ]
    }
   ],
   "source": [
    "# Augment generated samples by adding background noise and applying room impulse responses\n",
    "from data import augment_clips, filter_audio_paths\n",
    "\n",
    "config[\"rirs_dir\"] = os.path.abspath(config[\"rirs_dir\"])\n",
    "config[\"audioset_clips_dir\"] = os.path.abspath(config[\"audioset_clips_dir\"])\n",
    "config[\"fma_clips_dir\"] = os.path.abspath(config[\"fma_clips_dir\"])\n",
    "config[\"fsd_clips_dir\"] = os.path.abspath(config[\"fsd_clips_dir\"])\n",
    "config[\"features_output_dir\"] = os.path.abspath(config[\"features_output_dir\"])\n",
    "config[\"audio_samples_per_clip\"] = int((config[\"spectrogram_duration_ms\"])*config[\"sample_rate_hz\"]/1000) # ms * herz *1/(1000ms) = # of samples\n",
    "\n",
    "print(\"audio_samples_per_clip: \", config[\"audio_samples_per_clip\"])\n",
    "\n",
    "if not os.path.exists(config[\"features_output_dir\"]):\n",
    "    os.mkdir(config[\"features_output_dir\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering with max_duration_sec:  1.43  sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300000it [00:01, 237471.82it/s]\n",
      "20000it [00:00, 252475.00it/s]\n",
      "20000it [00:00, 255032.58it/s]\n",
      "252296it [00:01, 247808.76it/s]\n",
      "20000it [00:00, 247428.21it/s]\n",
      "20000it [00:00, 253051.54it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_duration_sec = config[\"clip_duration_ms\"]/1000.0\n",
    "print(\"filtering with max_duration_sec: \", max_duration_sec, \" sec\")\n",
    "\n",
    "spectrogram_duration_sec = config[\"spectrogram_duration_ms\"]/1000.0\n",
    "jitter_s = config[\"end_jitter_ms\"]/1000.0\n",
    "\n",
    "positive_tts_clips_train, durations = filter_audio_paths([positive_train_output_dir],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"size\")\n",
    "positive_tts_clips_test, durations = filter_audio_paths([positive_test_output_dir],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"size\")\n",
    "positive_tts_clips_validation, durations = filter_audio_paths([positive_validation_output_dir],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"size\")\n",
    "\n",
    "negative_tts_clips_train, durations = filter_audio_paths([negative_train_output_dir],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"size\")\n",
    "negative_tts_clips_test, durations = filter_audio_paths([negative_test_output_dir],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"size\")\n",
    "negative_tts_clips_validation, durations = filter_audio_paths([negative_validation_output_dir],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"size\")\n",
    "\n",
    "rir_paths = [str(i) for i in Path(config[\"rirs_dir\"]).glob(config[\"rirs_glob\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if restore_features == False:\n",
    "  !rm -rf ./training_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of resource variables the model uses =  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RecordingMicroAllocator] Arena allocation total 11800 bytes\n",
      "[RecordingMicroAllocator] Arena allocation head 3040 bytes\n",
      "[RecordingMicroAllocator] Arena allocation tail 8760 bytes\n",
      "[RecordingMicroAllocator] 'TfLiteEvalTensor data' used 1032 bytes with alignment overhead (requested 1032 bytes for 43 allocations)\n",
      "[RecordingMicroAllocator] 'Persistent TfLiteTensor data' used 224 bytes with alignment overhead (requested 224 bytes for 2 tensors)\n",
      "[RecordingMicroAllocator] 'Persistent buffer data' used 4988 bytes with alignment overhead (requested 4872 bytes for 19 allocations)\n",
      "[RecordingMicroAllocator] 'NodeAndRegistration struct' used 1936 bytes with alignment overhead (requested 1936 bytes for 22 NodeAndRegistration structs)\n"
     ]
    }
   ],
   "source": [
    "# the following swithc uses the tflite preprocessor (same one used in the embedded device) as opposed of using\n",
    "# the microfrontend s/w based one\n",
    "tflite_prep = True\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.lite.experimental.microfrontend.python.ops import audio_microfrontend_op as frontend_op\n",
    "from tflite_micro.python.tflite_micro import runtime\n",
    "\n",
    "preprocessor_model = runtime.Interpreter.from_file(\"./audio_preprocessor_int8.tflite\")\n",
    "input_details = preprocessor_model.get_input_details(0)\n",
    "output_details = preprocessor_model.get_output_details(0)\n",
    "preprocessor_model.print_allocations()\n",
    "\n",
    "class VectorSplitter:\n",
    "    def __init__(self, chunk_size=480):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.remainder = np.zeros(160)\n",
    "\n",
    "    def split_into_chunks(self, vector):\n",
    "        chunks = []\n",
    "        #print(f\"Splitting vector of size {len(vector)}, stored remainder is of size {len(self.remainder)}\")\n",
    "        vector = np.concatenate((self.remainder, vector))\n",
    "        #print(f\"Concatenated vector of size {len(vector)}\")\n",
    "        i = 0\n",
    "        while i + self.chunk_size <= len(vector):\n",
    "        #for i in range(0, len(vector), self.chunk_size):\n",
    "            chunk = vector[i:i + self.chunk_size]\n",
    "            chunks.append(chunk)\n",
    "            #print(\"append chunk \", chunk.shape, \" i:\", i)\n",
    "            #self.remainder = vector[-160:]\n",
    "            #print(\"Remainder:\", self.remainder.shape)\n",
    "            i += 320\n",
    "        #if i < len(vector):\n",
    "        self.remainder = vector[i:]\n",
    "        #print(\"end of vector Remainder:\", self.remainder.shape)\n",
    "        return chunks\n",
    "\n",
    "splitter = VectorSplitter()\n",
    "def get_features(input):\n",
    "    if len(input) != 480:\n",
    "        raise ValueError(\"Input must be of size 480\")\n",
    "        return\n",
    "    preprocessor_model.set_input(input.reshape([1,480]).astype(np.int16), 0)\n",
    "    preprocessor_model.invoke()\n",
    "    return preprocessor_model.get_output(0)\n",
    "\n",
    "# this generates the MEL spectrogram features for a given clip\n",
    "def generate_features_for_clip(clip):\n",
    "\n",
    "    if tflite_prep == True:\n",
    "        chunks =  splitter.split_into_chunks(clip)\n",
    "        matrix = np.array([get_features(chunk) for chunk in chunks])\n",
    "        return matrix\n",
    "    else:\n",
    "        micro_frontend = frontend_op.audio_microfrontend(\n",
    "            tf.convert_to_tensor(clip),\n",
    "            sample_rate=16000,\n",
    "            window_size=30,\n",
    "            window_step=20,\n",
    "            num_channels=40,\n",
    "            upper_band_limit=7500,\n",
    "            lower_band_limit=125,\n",
    "            enable_pcan=True,\n",
    "            min_signal_remaining=0.05,\n",
    "            out_scale=1,\n",
    "            out_type=tf.float32)\n",
    "        output = tf.multiply(micro_frontend, 0.0390625)\n",
    "        return output.numpy()\n",
    "\n",
    "def clip_features_generator(generator):\n",
    "    for data in generator:\n",
    "        for clip in data:\n",
    "            yield generate_features_for_clip(clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object augment_clips at 0x7f79c2dd8a00>\n",
      "Generating  299981  features into  /home/lior/Custom-MicroWakeWord-Generator/training_features/positive_tts_samples/training/wakeword_batch_0_mmap\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9062416f77b143dd9c486f1fab846c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating  19999  features into  /home/lior/Custom-MicroWakeWord-Generator/training_features/positive_tts_samples/testing/wakeword_batch_0_mmap\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391d5d24945d4f73a71bdb46fef16799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating  19998  features into  /home/lior/Custom-MicroWakeWord-Generator/training_features/positive_tts_samples/validation/wakeword_batch_0_mmap\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8181ffc93d34ad1b2a0b39b14cd470c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating  250124  features into  /home/lior/Custom-MicroWakeWord-Generator/training_features/negative_tts_samples/training/unknown_batch_0_mmap\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded613c98e1e4051832e6b728d647dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating  19886  features into  /home/lior/Custom-MicroWakeWord-Generator/training_features/negative_tts_samples/testing/unknown_batch_0_mmap\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4873caa60f064a7cb3d4abcf26922787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating  19898  features into  /home/lior/Custom-MicroWakeWord-Generator/training_features/negative_tts_samples/validation/unknown_batch_0_mmap\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49030002e7cc492696bd7623b3014dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###################################\n",
    "# Generate features for TTS samples\n",
    "###################################\n",
    "for i in range(0,1):\n",
    "    positive_train_generator = augment_clips(positive_tts_clips_train,\n",
    "                        total_length=config[\"audio_samples_per_clip\"],\n",
    "                        batch_size=config[\"augment_batch_size\"],\n",
    "                        background_clip_paths=[config[\"audioset_clips_dir\"], config[\"fma_clips_dir\"], config[\"fsd_clips_dir\"]],\n",
    "                        RIR_paths=rir_paths)\n",
    "    positive_test_generator = augment_clips(positive_tts_clips_test,\n",
    "                        total_length=config[\"audio_samples_per_clip\"],\n",
    "                        batch_size=config[\"augment_batch_size\"],\n",
    "                        background_clip_paths=[config[\"audioset_clips_dir\"], config[\"fma_clips_dir\"], config[\"fsd_clips_dir\"]],\n",
    "                        RIR_paths=rir_paths)\n",
    "    positive_validation_generator = augment_clips(positive_tts_clips_validation,\n",
    "                        total_length=config[\"audio_samples_per_clip\"],\n",
    "                        batch_size=config[\"augment_batch_size\"],\n",
    "                        background_clip_paths=[config[\"audioset_clips_dir\"], config[\"fma_clips_dir\"], config[\"fsd_clips_dir\"]],\n",
    "                        RIR_paths=rir_paths)\n",
    "\n",
    "    negative_train_generator = augment_clips(negative_tts_clips_train,\n",
    "                        total_length=config[\"audio_samples_per_clip\"],\n",
    "                        batch_size=config[\"augment_batch_size\"],\n",
    "                        background_clip_paths=[config[\"audioset_clips_dir\"], config[\"fma_clips_dir\"], config[\"fsd_clips_dir\"]],\n",
    "                        RIR_paths=rir_paths)\n",
    "    negative_test_generator = augment_clips(negative_tts_clips_test,\n",
    "                        total_length=config[\"audio_samples_per_clip\"],\n",
    "                        batch_size=config[\"augment_batch_size\"],\n",
    "                        background_clip_paths=[config[\"audioset_clips_dir\"], config[\"fma_clips_dir\"], config[\"fsd_clips_dir\"]],\n",
    "                        RIR_paths=rir_paths)\n",
    "    negative_validation_generator = augment_clips(negative_tts_clips_validation,\n",
    "                        total_length=config[\"audio_samples_per_clip\"],\n",
    "                        batch_size=config[\"augment_batch_size\"],\n",
    "                        background_clip_paths=[config[\"audioset_clips_dir\"], config[\"fma_clips_dir\"], config[\"fsd_clips_dir\"]],\n",
    "                        RIR_paths=rir_paths)\n",
    "\n",
    "    print(positive_train_generator)\n",
    "\n",
    "\n",
    "    dataset_name = \"positive_tts_samples\"\n",
    "    pos_dir_fname = \"wakeword\"\n",
    "    augmented_positive_test_directory = os.path.join(config[\"features_output_dir\"] + \"/\" + dataset_name + \"/testing\", pos_dir_fname)\n",
    "    augmented_positive_validation_directory = os.path.join(config[\"features_output_dir\"] + \"/\" + dataset_name + \"/validation\", pos_dir_fname)\n",
    "    augmented_positive_train_directory = os.path.join(config[\"features_output_dir\"] + \"/\" + dataset_name + \"/training\", pos_dir_fname)\n",
    "\n",
    "    dataset_name = \"negative_tts_samples\"\n",
    "    neg_dir_fname = \"unknown\"\n",
    "    augmented_negative_test_directory = os.path.join(config[\"features_output_dir\"] + \"/\" + dataset_name + \"/testing\", neg_dir_fname)\n",
    "    augmented_negative_validation_directory = os.path.join(config[\"features_output_dir\"] + \"/\" + dataset_name + \"/validation\", neg_dir_fname)\n",
    "    augmented_negative_train_directory = os.path.join(config[\"features_output_dir\"] + \"/\" + dataset_name + \"/training\", neg_dir_fname)\n",
    "\n",
    "    generator_outputs = [\n",
    "                         [positive_train_generator, augmented_positive_train_directory, len(positive_tts_clips_train)],\n",
    "                         [positive_test_generator, augmented_positive_test_directory,len(positive_tts_clips_test)],\n",
    "                         [positive_validation_generator, augmented_positive_validation_directory, len(positive_tts_clips_validation)],\n",
    "                         [negative_train_generator, augmented_negative_train_directory, len(negative_tts_clips_train)],\n",
    "                         [negative_test_generator, augmented_negative_test_directory, len(negative_tts_clips_test)],\n",
    "                         [negative_validation_generator, augmented_negative_validation_directory, len(negative_tts_clips_validation)]\n",
    "                        ]\n",
    "\n",
    "\n",
    "    for [generator, output_directory, n_total] in generator_outputs:\n",
    "\n",
    "        output_directory = output_directory + '_batch_'+str(i)+'_mmap'\n",
    "        print(\"Generating \", n_total, \" features into \", output_directory)\n",
    "        if not os.path.exists(output_directory):\n",
    "            os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "        RaggedMmap.from_generator(out_dir=output_directory,sample_generator=clip_features_generator(generator),batch_size=1024,verbose=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found  0  audio files\n",
      "test_output_dir: /home/lior/Custom-MicroWakeWord-Generator/training_features/but_reverdb_silence/testing/but_reverdb_silence_test_23840ms_mmap\n",
      "validation_output_dir: /home/lior/Custom-MicroWakeWord-Generator/training_features/but_reverdb_silence/validation/but_reverdb_silence_validation_23840ms_mmap\n",
      "train_output_dir: /home/lior/Custom-MicroWakeWord-Generator/training_features/but_reverdb_silence/training/but_reverdb_silence_training__mmap\n",
      "Generating features for test set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16435a1fce2840db81665db8ae1a1982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating features for validation set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f4590623bc4c7d84ff25af5ac3a667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating features for training set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cae6999b5f445719ff02e8724f1e17f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "### NOT FUNCTIONAL #### contains RIR only \n",
    "\n",
    "\n",
    "# Generates features for background silence datasets\n",
    "# Input files do not need to be pre-converted to wav\n",
    "# Test and validations sets are truncated to the clip duration for consistent testing\n",
    "# Training sets convert the entire clip; the training process randomly truncates it each time used\n",
    "\n",
    "import datasets\n",
    "import sys\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from mmap_ninja.ragged import RaggedMmap\n",
    "from data import truncate_clip\n",
    "\n",
    "logging.getLogger('numba').setLevel(logging.WARNING)\n",
    "\n",
    "# from http://merlin.fit.vutbr.cz/ReverbDB/BUT_ReverbDB_rel_19_06_RIR-Only.tgz\n",
    "\n",
    "path_to_audio_dataset = \"datasets/BUT_ReverbDB_rel_19_06_RIR-Only\"\n",
    "audio_dataset_glob = \"**/silence/*.wav\"\n",
    "dataset_name = \"but_reverdb_silence\"\n",
    "\n",
    "audioset_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(path_to_audio_dataset).glob(audio_dataset_glob)]})\n",
    "audioset_dataset = audioset_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
    "print(\"found \", len(audioset_dataset), \" audio files\")\n",
    "train_testvalid = audioset_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "test_validate = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "def features_generator(set):\n",
    "    if set == 'train':\n",
    "        for row in train_testvalid['train']:\n",
    "            if len(row['audio']['array']) < config[\"audio_samples_per_clip\"]:  # ensure clip has at least as many samples as needed\n",
    "                continue\n",
    "            yield generate_features_for_clip((row['audio']['array']*32767).astype(np.int16))\n",
    "    elif set == 'test':\n",
    "        for row in test_validate['test']:\n",
    "            if len(row['audio']['array']) < config[\"audio_samples_per_clip\"]:  # ensure clip has at least as many samples as needed\n",
    "                continue\n",
    "\n",
    "            # Truncate for a consistent test set\n",
    "            truncated = truncate_clip((row['audio']['array']*32767).astype(np.int16), config[\"audio_samples_per_clip\"], \"random\")\n",
    "            yield generate_features_for_clip(truncated)\n",
    "    elif set == 'validate':\n",
    "        for row in test_validate['train']:\n",
    "            if len(row['audio']['array']) < config[\"audio_samples_per_clip\"]: # ensure clip has at least as many samples as needed\n",
    "                continue\n",
    "\n",
    "            # Truncate for a consistent validation set\n",
    "            truncated = truncate_clip((row['audio']['array']*32767).astype(np.int16), config[\"audio_samples_per_clip\"], \"random\")\n",
    "            yield generate_features_for_clip(truncated)\n",
    "\n",
    "\n",
    "test_dir_fname = dataset_name + '_test_' + str(config[\"audio_samples_per_clip\"]) + 'ms_mmap'\n",
    "validation_dir_fname = dataset_name + '_validation_' + str(config[\"audio_samples_per_clip\"]) + 'ms_mmap'\n",
    "train_dir_fname = dataset_name + '_training_' + '_mmap'\n",
    "\n",
    "\n",
    "test_output_dir = os.path.join(config[\"features_output_dir\"] + \"/\" + dataset_name + \"/testing\", test_dir_fname)\n",
    "validation_output_dir = os.path.join(config[\"features_output_dir\"] + \"/\" + dataset_name + \"/validation\", validation_dir_fname)\n",
    "train_output_dir = os.path.join(config[\"features_output_dir\"] + \"/\" + dataset_name + \"/training\", train_dir_fname)\n",
    "\n",
    "if not os.path.exists(test_output_dir):\n",
    "    os.makedirs(test_output_dir)\n",
    "if not os.path.exists(validation_output_dir):\n",
    "    os.makedirs(validation_output_dir)\n",
    "if not os.path.exists(train_output_dir):\n",
    "    os.makedirs(train_output_dir)\n",
    "\n",
    "print(\"test_output_dir:\", test_output_dir)\n",
    "print(\"validation_output_dir:\", validation_output_dir)\n",
    "print(\"train_output_dir:\", train_output_dir)\n",
    "\n",
    "print(\"Generating features for test set\")\n",
    "RaggedMmap.from_generator(out_dir=test_output_dir,\n",
    "                          sample_generator=features_generator('test'),\n",
    "                          batch_size=1024,\n",
    "                          verbose=True)\n",
    "print(\"Generating features for validation set\")\n",
    "RaggedMmap.from_generator(out_dir=validation_output_dir,\n",
    "                          sample_generator=features_generator('validate'),\n",
    "                          batch_size=1024,\n",
    "                          verbose=True)\n",
    "print(\"Generating features for training set\")\n",
    "RaggedMmap.from_generator(out_dir=train_output_dir,\n",
    "                          sample_generator=features_generator('train'),\n",
    "                          batch_size=1024,\n",
    "                          verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_output_dir: /home/lior/Custom-MicroWakeWord-Generator/training_features/wham_noise/testing/wham_noise_test_23840ms_mmap\n",
      "validation_output_dir: /home/lior/Custom-MicroWakeWord-Generator/training_features/wham_noise/validation/wham_noise_validation_23840ms_mmap\n",
      "train_output_dir: /home/lior/Custom-MicroWakeWord-Generator/training_features/wham_noise/training/wham_noise_training__mmap\n",
      "Generating features for test set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a46d85d75346a681bd71b5504fe77b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating features for validation set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be654efaf2154f6cb6ead42bf55db63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating features for training set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b3d9be6d4445d58c114c223956a802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done generating features for background noise datasets - copying them to mWW source dataset directory\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# download background noise\n",
    "# from WHAM from http://wham.whisper.ai/\n",
    "###################################\n",
    "\n",
    "path_to_audio_dataset = \"datasets/wham_noise\"\n",
    "audio_dataset_glob = \"*.wav\"\n",
    "dataset_name = \"wham_noise\"\n",
    "\n",
    "audioset_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(path_to_audio_dataset).glob(audio_dataset_glob)]})\n",
    "audioset_dataset = audioset_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
    "\n",
    "train_testvalid = audioset_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "test_validate = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "def features_generator(set):\n",
    "    if set == 'train':\n",
    "        for row in train_testvalid['train']:\n",
    "            if len(row['audio']['array']) < config[\"audio_samples_per_clip\"]:  # ensure clip has at least as many samples as needed\n",
    "                continue\n",
    "            yield generate_features_for_clip((row['audio']['array']*32767).astype(np.int16))\n",
    "    elif set == 'test':\n",
    "        for row in test_validate['test']:\n",
    "            if len(row['audio']['array']) < config[\"audio_samples_per_clip\"]:  # ensure clip has at least as many samples as needed\n",
    "                continue\n",
    "\n",
    "            # Truncate for a consistent test set\n",
    "            truncated = truncate_clip((row['audio']['array']*32767).astype(np.int16), config[\"audio_samples_per_clip\"], \"random\")\n",
    "            yield generate_features_for_clip(truncated)\n",
    "    elif set == 'validate':\n",
    "        for row in test_validate['train']:\n",
    "            if len(row['audio']['array']) < config[\"audio_samples_per_clip\"]: # ensure clip has at least as many samples as needed\n",
    "                continue\n",
    "\n",
    "            # Truncate for a consistent validation set\n",
    "            truncated = truncate_clip((row['audio']['array']*32767).astype(np.int16), config[\"audio_samples_per_clip\"], \"random\")\n",
    "            yield generate_features_for_clip(truncated)\n",
    "\n",
    "\n",
    "test_dir_fname = dataset_name + '_test_' + str(config[\"audio_samples_per_clip\"]) + 'ms_mmap'\n",
    "validation_dir_fname = dataset_name + '_validation_' + str(config[\"audio_samples_per_clip\"]) + 'ms_mmap'\n",
    "train_dir_fname = dataset_name + '_training_' + '_mmap'\n",
    "\n",
    "\n",
    "test_output_dir = os.path.join(config[\"features_output_dir\"] + \"/\" + dataset_name + \"/testing\", test_dir_fname)\n",
    "validation_output_dir = os.path.join(config[\"features_output_dir\"] + \"/\" + dataset_name + \"/validation\", validation_dir_fname)\n",
    "train_output_dir = os.path.join(config[\"features_output_dir\"] + \"/\" + dataset_name + \"/training\", train_dir_fname)\n",
    "\n",
    "if not os.path.exists(test_output_dir):\n",
    "    os.makedirs(test_output_dir)\n",
    "if not os.path.exists(validation_output_dir):\n",
    "    os.makedirs(validation_output_dir)\n",
    "if not os.path.exists(train_output_dir):\n",
    "    os.makedirs(train_output_dir)\n",
    "\n",
    "print(\"test_output_dir:\", test_output_dir)\n",
    "print(\"validation_output_dir:\", validation_output_dir)\n",
    "print(\"train_output_dir:\", train_output_dir)\n",
    "\n",
    "print(\"Generating features for test set\")\n",
    "RaggedMmap.from_generator(out_dir=test_output_dir,\n",
    "                          sample_generator=features_generator('test'),\n",
    "                          batch_size=1024,\n",
    "                          verbose=True)\n",
    "print(\"Generating features for validation set\")\n",
    "RaggedMmap.from_generator(out_dir=validation_output_dir,\n",
    "                          sample_generator=features_generator('validate'),\n",
    "                          batch_size=1024,\n",
    "                          verbose=True)\n",
    "print(\"Generating features for training set\")\n",
    "RaggedMmap.from_generator(out_dir=train_output_dir,\n",
    "                          sample_generator=features_generator('train'),\n",
    "                          batch_size=1024,\n",
    "                          verbose=True)\n",
    "\n",
    "print(\"Done generating features for background noise datasets - copying them to mWW source dataset directory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_output_dir: /home/lior/Custom-MicroWakeWord-Generator/training_features/dipco/testing_ambient/dipco_test_23840ms_mmap\n",
      "validation_output_dir: /home/lior/Custom-MicroWakeWord-Generator/training_features/dipco/validation_ambient/dipco_validation_23840ms_mmap\n",
      "train_output_dir: /home/lior/Custom-MicroWakeWord-Generator/training_features/dipco/training/dipco_training__mmap\n",
      "Generating features for test set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1fe7c36b3d4a668b4848bd0523dfd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating features for validation set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81e101d7d0a4dceb5290dd20d4571a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating features for training set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121f89f193c64048902783bf8c1b455c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done generating features for DIPCO noise datasets \n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# generate features for DIPCO noise\n",
    "###################################\n",
    "\n",
    "#\n",
    "path_to_audio_dataset = \"datasets/dipco\"\n",
    "audio_dataset_glob = \"*.wav\"\n",
    "dataset_name = \"dipco\"\n",
    "\n",
    "audioset_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(path_to_audio_dataset).glob(audio_dataset_glob)]})\n",
    "audioset_dataset = audioset_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
    "\n",
    "train_testvalid = audioset_dataset.train_test_split(test_size=0.8)\n",
    "\n",
    "test_validate = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "def features_generator(set):\n",
    "    if set == 'train':\n",
    "        for row in train_testvalid['train']:\n",
    "            if len(row['audio']['array']) < config[\"audio_samples_per_clip\"]:  # ensure clip has at least as many samples as needed\n",
    "                continue\n",
    "            yield generate_features_for_clip((row['audio']['array']*32767).astype(np.int16))\n",
    "    elif set == 'test':\n",
    "        for row in test_validate['test']:\n",
    "            if len(row['audio']['array']) < config[\"audio_samples_per_clip\"]:  # ensure clip has at least as many samples as needed\n",
    "                continue\n",
    "\n",
    "            # # Truncate for a consistent test set\n",
    "            # truncated = truncate_clip((row['audio']['array']*32767).astype(np.int16), config[\"audio_samples_per_clip\"], \"random\")\n",
    "            # yield generate_features_for_clip(truncated)\n",
    "            yield generate_features_for_clip((row['audio']['array']*32767).astype(np.int16))\n",
    "    elif set == 'validate':\n",
    "        for row in test_validate['train']:\n",
    "            if len(row['audio']['array']) < config[\"audio_samples_per_clip\"]: # ensure clip has at least as many samples as needed\n",
    "                continue\n",
    "\n",
    "            # # Truncate for a consistent validation set\n",
    "            # truncated = truncate_clip((row['audio']['array']*32767).astype(np.int16), config[\"audio_samples_per_clip\"], \"random\")\n",
    "            # yield generate_features_for_clip(truncated)\n",
    "            yield generate_features_for_clip((row['audio']['array']*32767).astype(np.int16))\n",
    "\n",
    "\n",
    "test_dir_fname = dataset_name + '_test_' + str(config[\"audio_samples_per_clip\"]) + 'ms_mmap'\n",
    "validation_dir_fname = dataset_name + '_validation_' + str(config[\"audio_samples_per_clip\"]) + 'ms_mmap'\n",
    "train_dir_fname = dataset_name + '_training_' + '_mmap'\n",
    "\n",
    "\n",
    "test_output_dir = os.path.join(config[\"features_output_dir\"] + \"/\" + dataset_name + \"/testing_ambient\", test_dir_fname)\n",
    "validation_output_dir = os.path.join(config[\"features_output_dir\"] + \"/\" + dataset_name + \"/validation_ambient\", validation_dir_fname)\n",
    "train_output_dir = os.path.join(config[\"features_output_dir\"] + \"/\" + dataset_name + \"/training\", train_dir_fname)\n",
    "\n",
    "if not os.path.exists(test_output_dir):\n",
    "    os.makedirs(test_output_dir)\n",
    "if not os.path.exists(validation_output_dir):\n",
    "    os.makedirs(validation_output_dir)\n",
    "if not os.path.exists(train_output_dir):\n",
    "    os.makedirs(train_output_dir)\n",
    "\n",
    "print(\"test_output_dir:\", test_output_dir)\n",
    "print(\"validation_output_dir:\", validation_output_dir)\n",
    "print(\"train_output_dir:\", train_output_dir)\n",
    "\n",
    "print(\"Generating features for test set\")\n",
    "RaggedMmap.from_generator(out_dir=test_output_dir,\n",
    "                          sample_generator=features_generator('test'),\n",
    "                          batch_size=1024,\n",
    "                          verbose=True)\n",
    "print(\"Generating features for validation set\")\n",
    "RaggedMmap.from_generator(out_dir=validation_output_dir,\n",
    "                          sample_generator=features_generator('validate'),\n",
    "                          batch_size=1024,\n",
    "                          verbose=True)\n",
    "print(\"Generating features for training set\")\n",
    "RaggedMmap.from_generator(out_dir=train_output_dir,\n",
    "                          sample_generator=features_generator('train'),\n",
    "                          batch_size=1024,\n",
    "                          verbose=True)\n",
    "\n",
    "print(\"Done generating features for DIPCO noise datasets \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:00, 44949.00it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 6355.01it/s]\n",
      "4it [00:00, 11992.29it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 4153.80it/s]\n",
      "4it [00:00, 11740.53it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 4356.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./datasets/captured_samples/positive_train/positive4058a60a3c714d0f98e9ebe3d1606164.wav', './datasets/captured_samples/positive_train/positive6430ab7269c34cd1b64e2bee610099d4.wav', './datasets/captured_samples/positive_train/positive7186a2b74f1c4bdfac48a47d7b51e1c8.wav', './datasets/captured_samples/positive_train/positive722e8929218e4707a66ea3a3de40ac67.wav', './datasets/captured_samples/positive_train/positive0da5fca5153246a8ae2e631f666622c8.wav', './datasets/captured_samples/positive_train/positive17bc63c2f2274366be22f956e0d84eff.wav', './datasets/captured_samples/positive_train/positivee23a131debd34c5faa7cc612349db651.wav', './datasets/captured_samples/positive_train/positivea41702dd97044a0cbe80d5f3f55a6473.wav', './datasets/captured_samples/positive_train/positive57044322985d4f3295cd19619fcc41e3.wav', './datasets/captured_samples/positive_train/positivef90b27b9fae6434c8deb523ade855ac0.wav', './datasets/captured_samples/positive_train/positivef9c8785ffede4d90ae7bb7213e98a6b8.wav', './datasets/captured_samples/positive_train/positive30f2603cad8b4635a30ffae83dab126d.wav', './datasets/captured_samples/positive_train/positivec07155da5f674062acb3c0cf6f635721.wav', './datasets/captured_samples/positive_train/positivec0d9c3506a41487fa549e77960f01686.wav', './datasets/captured_samples/positive_train/positive161cc61b98714e57bdb2dcd1b081dc2a.wav', './datasets/captured_samples/positive_train/positive396520232b91459a9966e0ce04cc80fc.wav']\n",
      "Generating  16  features into  /home/lior/Custom-MicroWakeWord-Generator/training_features/positive_voice_samples/training/wakeword_batch_0_mmap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10702e3d5c9642fbb08c4c54daa2f68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating  4  features into  /home/lior/Custom-MicroWakeWord-Generator/training_features/positive_voice_samples/testing/wakeword_batch_0_mmap\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a8bed793824af2a44a150ebc181ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating  4  features into  /home/lior/Custom-MicroWakeWord-Generator/training_features/positive_voice_samples/validation/wakeword_batch_0_mmap\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1fde3bff67497b9877bd41425bae05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###################################\n",
    "# generate features based on captured audio samples\n",
    "###################################\n",
    "\n",
    "max_duration_sec = 4\n",
    "positive_captured_clips_train, durations = filter_audio_paths(['./datasets/captured_samples/positive_train/'],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"header\")\n",
    "positive_captured_clips_test, durations = filter_audio_paths(['./datasets/captured_samples/positive_test/'],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"header\")\n",
    "positive_captured_clips_validation, durations = filter_audio_paths(['./datasets/captured_samples/positive_validation'],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"header\")\n",
    "\n",
    "audio_samples_per_clip = config[\"audio_samples_per_clip\"] #1.34*16000\n",
    "for i in range(0,1):\n",
    "\n",
    "    positive_train_generator = augment_clips(positive_captured_clips_train,\n",
    "                        total_length=audio_samples_per_clip,\n",
    "                        batch_size=config[\"augment_batch_size\"],\n",
    "                        background_clip_paths=[config[\"audioset_clips_dir\"], config[\"fma_clips_dir\"], config[\"fsd_clips_dir\"]],\n",
    "                        RIR_paths=rir_paths)\n",
    "    positive_test_generator = augment_clips(positive_captured_clips_test,\n",
    "                        total_length=audio_samples_per_clip,\n",
    "                        batch_size=config[\"augment_batch_size\"],\n",
    "                        background_clip_paths=[config[\"audioset_clips_dir\"], config[\"fma_clips_dir\"], config[\"fsd_clips_dir\"]],\n",
    "                        RIR_paths=rir_paths)\n",
    "    positive_validation_generator = augment_clips(positive_captured_clips_validation,\n",
    "                        total_length=audio_samples_per_clip,\n",
    "                        batch_size=config[\"augment_batch_size\"],\n",
    "                        background_clip_paths=[config[\"audioset_clips_dir\"], config[\"fma_clips_dir\"], config[\"fsd_clips_dir\"]],\n",
    "                        RIR_paths=rir_paths)\n",
    "\n",
    "\n",
    "\n",
    "    print(positive_captured_clips_train)\n",
    "\n",
    "\n",
    "    dataset_name = \"positive_voice_samples\"\n",
    "    pos_dir_fname = \"wakeword\"\n",
    "    augmented_positive_test_directory = os.path.join(config[\"features_output_dir\"] + \"/\" + dataset_name + \"/testing\", pos_dir_fname)\n",
    "    augmented_positive_validation_directory = os.path.join(config[\"features_output_dir\"] + \"/\" + dataset_name + \"/validation\", pos_dir_fname)\n",
    "    augmented_positive_train_directory = os.path.join(config[\"features_output_dir\"] + \"/\" + dataset_name + \"/training\", pos_dir_fname)\n",
    "\n",
    "    generator_outputs = [\n",
    "                         [positive_train_generator, augmented_positive_train_directory, len(positive_captured_clips_train)],\n",
    "                         [positive_test_generator, augmented_positive_test_directory,len(positive_captured_clips_test)],\n",
    "                         [positive_validation_generator, augmented_positive_validation_directory, len(positive_captured_clips_validation)],\n",
    "                        ]\n",
    "\n",
    "\n",
    "    for [generator, output_directory, n_total] in generator_outputs:\n",
    "\n",
    "        output_directory = output_directory + '_batch_'+str(i)+'_mmap'\n",
    "        print(\"Generating \", n_total, \" features into \", output_directory)\n",
    "        if not os.path.exists(output_directory):\n",
    "            os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "        RaggedMmap.from_generator(out_dir=output_directory,sample_generator=clip_features_generator(generator),batch_size=1,verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1999it [00:00, 242366.12it/s]\n",
      "100%|██████████| 1999/1999 [00:00<00:00, 13330.62it/s]\n",
      "1999it [00:00, 237109.07it/s]\n",
      "100%|██████████| 1999/1999 [00:00<00:00, 13468.44it/s]\n",
      "1939it [00:00, 249723.81it/s]\n",
      "100%|██████████| 1939/1939 [00:00<00:00, 11139.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting clips with noise and room impulse response\n",
      "Generating  1999  features into  /home/lior/Custom-MicroWakeWord-Generator/training_features/positive_cloned_samples/training/wakeword_batch_0_mmap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71918df60ab48909a1b58ed32de3b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating  1999  features into  /home/lior/Custom-MicroWakeWord-Generator/training_features/positive_cloned_samples/testing/wakeword_batch_0_mmap\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59b819c24a446fe95239c8275c037e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating  1939  features into  /home/lior/Custom-MicroWakeWord-Generator/training_features/positive_cloned_samples/validation/wakeword_batch_0_mmap\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c39b1b9f9d84e8ebf647e7632679a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###################################\n",
    "# generate features based on COQUI'ed captured audio samples\n",
    "###################################\n",
    "\n",
    "max_duration_sec = 3\n",
    "positive_cloned_clips_train, durations = filter_audio_paths(['./datasets/cloned_samples/positive_train/'],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"header\")\n",
    "positive_cloned_clips_test, durations = filter_audio_paths(['./datasets/cloned_samples/positive_test/'],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"header\")\n",
    "positive_cloned_clips_validation, durations = filter_audio_paths(['./datasets/cloned_samples/positive_validation'],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"header\")\n",
    "\n",
    "audio_samples_per_clip = 2*16000\n",
    "for i in range(0,1):\n",
    "    print(\"Augmenting clips with noise and room impulse response\")\n",
    "\n",
    "    positive_train_generator = augment_clips(positive_cloned_clips_train,\n",
    "                        total_length=audio_samples_per_clip,\n",
    "                        batch_size=config[\"augment_batch_size\"],\n",
    "                        background_clip_paths=[config[\"audioset_clips_dir\"], config[\"fma_clips_dir\"], config[\"fsd_clips_dir\"]],\n",
    "                        RIR_paths=rir_paths)\n",
    "    positive_test_generator = augment_clips(positive_cloned_clips_test,\n",
    "                        total_length=audio_samples_per_clip,\n",
    "                        batch_size=config[\"augment_batch_size\"],\n",
    "                        background_clip_paths=[config[\"audioset_clips_dir\"], config[\"fma_clips_dir\"], config[\"fsd_clips_dir\"]],\n",
    "                        RIR_paths=rir_paths)\n",
    "    positive_validation_generator = augment_clips(positive_cloned_clips_validation,\n",
    "                        total_length=audio_samples_per_clip,\n",
    "                        batch_size=config[\"augment_batch_size\"],\n",
    "                        background_clip_paths=[config[\"audioset_clips_dir\"], config[\"fma_clips_dir\"], config[\"fsd_clips_dir\"]],\n",
    "                        RIR_paths=rir_paths)\n",
    "\n",
    "\n",
    "\n",
    "    dataset_name = \"positive_cloned_samples\"\n",
    "    pos_dir_fname = \"wakeword\"\n",
    "    augmented_positive_test_directory = os.path.join(config[\"features_output_dir\"] + \"/\" + dataset_name + \"/testing\", pos_dir_fname)\n",
    "    augmented_positive_validation_directory = os.path.join(config[\"features_output_dir\"] + \"/\" + dataset_name + \"/validation\", pos_dir_fname)\n",
    "    augmented_positive_train_directory = os.path.join(config[\"features_output_dir\"] + \"/\" + dataset_name + \"/training\", pos_dir_fname)\n",
    "\n",
    "    generator_outputs = [\n",
    "                         [positive_train_generator, augmented_positive_train_directory, len(positive_cloned_clips_train)],\n",
    "                         [positive_test_generator, augmented_positive_test_directory,len(positive_cloned_clips_test)],\n",
    "                         [positive_validation_generator, augmented_positive_validation_directory, len(positive_cloned_clips_validation)],\n",
    "                        ]\n",
    "\n",
    "\n",
    "    for [generator, output_directory, n_total] in generator_outputs:\n",
    "\n",
    "        output_directory = output_directory + '_batch_'+str(i)+'_mmap'\n",
    "        print(\"Generating \", n_total, \" features into \", output_directory)\n",
    "        if not os.path.exists(output_directory):\n",
    "            os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "        RaggedMmap.from_generator(out_dir=output_directory,sample_generator=clip_features_generator(generator),batch_size=4,verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [00:00, 273417.70it/s]\n",
      "100%|██████████| 100000/100000 [00:07<00:00, 14230.11it/s]\n",
      "1000it [00:00, 249067.93it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 14433.40it/s]\n",
      "1000it [00:00, 243911.61it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 14516.18it/s]\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# generate features based on COQUI'ed captured audio samples\n",
    "###################################\n",
    "\n",
    "max_duration_sec = 1.5\n",
    "positive_pronounciation_clips_train, durations = filter_audio_paths(['./datasets/positive_more_pronunciations/alexa/positive_train/'],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"header\")\n",
    "positive_pronounciation_clips_test, durations = filter_audio_paths(['./datasets/positive_more_pronunciations/alexa/positive_test/'],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"header\")\n",
    "positive_pronounciation_clips_validation, durations = filter_audio_paths(['./datasets/positive_more_pronunciations/alexa/positive_validation'],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"header\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting clips with noise and room impulse response\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Augmenting clips with noise and room impulse response\")\n",
    "audio_samples_per_clip = config[\"audio_samples_per_clip\"]\n",
    "positive_train_generator = augment_clips(positive_pronounciation_clips_train,\n",
    "                    total_length=audio_samples_per_clip,\n",
    "                    batch_size=config[\"augment_batch_size\"],\n",
    "                    background_clip_paths=[config[\"audioset_clips_dir\"], config[\"fma_clips_dir\"], config[\"fsd_clips_dir\"]],\n",
    "                    RIR_paths=rir_paths)\n",
    "positive_test_generator = augment_clips(positive_pronounciation_clips_test,\n",
    "                    total_length=audio_samples_per_clip,\n",
    "                    batch_size=config[\"augment_batch_size\"],\n",
    "                    background_clip_paths=[config[\"audioset_clips_dir\"], config[\"fma_clips_dir\"], config[\"fsd_clips_dir\"]],\n",
    "                    RIR_paths=rir_paths)\n",
    "positive_validation_generator = augment_clips(positive_pronounciation_clips_validation,\n",
    "                    total_length=audio_samples_per_clip,\n",
    "                    batch_size=config[\"augment_batch_size\"],\n",
    "                    background_clip_paths=[config[\"audioset_clips_dir\"], config[\"fma_clips_dir\"], config[\"fsd_clips_dir\"]],\n",
    "                    RIR_paths=rir_paths)\n",
    "\n",
    "\n",
    "\n",
    "dataset_name = \"positive_pronounciation_samples\"\n",
    "pos_dir_fname = \"wakeword\"\n",
    "augmented_positive_test_directory = os.path.join(config[\"features_output_dir\"] + \"/\" + dataset_name + \"/testing\", pos_dir_fname)\n",
    "augmented_positive_validation_directory = os.path.join(config[\"features_output_dir\"] + \"/\" + dataset_name + \"/validation\", pos_dir_fname)\n",
    "augmented_positive_train_directory = os.path.join(config[\"features_output_dir\"] + \"/\" + dataset_name + \"/training\", pos_dir_fname)\n",
    "\n",
    "generator_outputs = [\n",
    "                        [positive_train_generator, augmented_positive_train_directory, len(positive_pronounciation_clips_train)],\n",
    "                        [positive_test_generator, augmented_positive_test_directory,len(positive_pronounciation_clips_test)],\n",
    "                        [positive_validation_generator, augmented_positive_validation_directory, len(positive_pronounciation_clips_validation)],\n",
    "                    ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating  98329  features into  /home/lior/Custom-MicroWakeWord-Generator/training_features/positive_pronounciation_samples/training/wakeword_batch_0_mmap\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fadb0b0804e4a3bb14461cb6ae6a057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating  985  features into  /home/lior/Custom-MicroWakeWord-Generator/training_features/positive_pronounciation_samples/testing/wakeword_batch_0_mmap\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14dab23cf2744889fd15932e6bc65b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating  990  features into  /home/lior/Custom-MicroWakeWord-Generator/training_features/positive_pronounciation_samples/validation/wakeword_batch_0_mmap\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c971f1690e24adeb85f629166b6a174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for [generator, output_directory, n_total] in generator_outputs:\n",
    "\n",
    "    output_directory = output_directory + '_batch_'+str(i)+'_mmap'\n",
    "    print(\"Generating \", n_total, \" features into \", output_directory)\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    RaggedMmap.from_generator(out_dir=output_directory,sample_generator=clip_features_generator(generator),batch_size=100,verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if running_on_colab:\n",
    "    # copy training feastures dir to GDrive for training on the next notebook\n",
    "    # backup generated samples\n",
    "    !tar -cvf ./training_features_$(date +%Y%m%d_%H%M%S).tar ./training_features\n",
    "    !cp ./training_features*.tar /content/drive/MyDrive/ColabNotebooks/VoiceAssistant/microWakeWord/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus:\n",
    "\n",
    "Visualizations of features: choose a sample ID to see its features from the sample itself *or* the version from the mmap which is augmented and is longer to account for initial condiftions inside the tflie based feature generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmap_ninja import RaggedMmap\n",
    "generated_samples_features = RaggedMmap('./training_features/positive_tts_samples/training/wakeword_batch_0_mmap')\n",
    "generated_samples_features = RaggedMmap('./training_features/positive_voice_samples/training/wakeword_batch_0_mmap')\n",
    "sample_id = 0\n",
    "print(\"There are \", len(generated_samples_features), \" samples in the mmap dataset\")\n",
    "print(\"There are \", len(positive_captured_clips_train), \" wav filenames in the original dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmap_ninja import RaggedMmap\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "mmap_fname = './training_features/positive_voice_samples/training/wakeword_batch_0_mmap'\n",
    "fnames_list = positive_captured_clips_train\n",
    "sample_id = 0\n",
    "\n",
    "def show_feature(\n",
    "        mmap_fname = './training_features/positive_voice_samples/training/wakeword_batch_0_mmap',\n",
    "        fnames_list = positive_captured_clips_train,\n",
    "        sample_id = 0):\n",
    "\n",
    "    filename = fnames_list[sample_id]\n",
    "    samples, sr = librosa.load(filename, sr=16000)\n",
    "    # Play the audio\n",
    "    audio = ipd.Audio(filename, rate=sr)\n",
    "    ipd.display(audio)\n",
    "    # plot the features from the wav itself\n",
    "    samples = (samples*32767).astype(np.int16)\n",
    "    filename = positive_captured_clips_train[sample_id]\n",
    "    samples, sr = librosa.load(filename, sr=16000)\n",
    "    samples = (samples*32767).astype(np.int16)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(2, 1, 1)  # 2 rows, 1 column, second plot\n",
    "    original_mfcc = generate_features_for_clip(samples).T\n",
    "    plt.imshow(original_mfcc,  interpolation='nearest')\n",
    "    plt.colorbar(format='%+2.0f')\n",
    "    plt.title('original_mfcc')\n",
    "\n",
    "    # plot the features from the mmap file\n",
    "    generated_samples_features = RaggedMmap(mmap_fname)\n",
    "    plt.subplot(2, 1, 2)  # 2 rows, 1 column, first plot\n",
    "    uagmented_mfcc = generated_samples_features[sample_id].T\n",
    "    plt.imshow(uagmented_mfcc)\n",
    "    plt.colorbar(format='%+2.0f')\n",
    "    plt.title('augmented_mfcc')\n",
    "    # Create the second plot\n",
    "\n",
    "\n",
    "    # Show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#show_feature()\n",
    "show_feature('./training_features/positive_tts_samples/training/wakeword_batch_0_mmap', positive_tts_clips_train, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
