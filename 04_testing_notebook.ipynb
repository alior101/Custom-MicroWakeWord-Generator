{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC7e0bC587b6"
   },
   "source": [
    "The notebook load the dataset from the Google drive and does inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all the required packages (borrowed from openWakeWord's automatic training notebook)\n",
    "running_on_colab = False\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Running on CoLab')\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    running_on_colab = True\n",
    "    restore_dataset = True\n",
    "    restore_features = True\n",
    "else:\n",
    "    print('Not running on CoLab')\n",
    "    restore_dataset = False\n",
    "    restore_features = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if running_on_colab:\n",
    "    # restoring previous trained model for finetuning from previously generated features\n",
    "    trained_model_filename = \"/content/drive/MyDrive/ColabNotebooks/VoiceAssistant/microWakeWord/trained_models_20240421_164022.tar\"\n",
    "    !cp {trained_model_filename}  .\n",
    "\n",
    "    model_filename = os.path.basename(trained_model_filename)\n",
    "    !tar -xvf {model_filename}\n",
    "\n",
    "if os.path.exists('audio_preprocessor_int8.tflite') == False:\n",
    "    !wget https://github.com/tensorflow/tflite-micro/raw/main/tensorflow/lite/micro/examples/micro_speech/models/audio_preprocessor_int8.tflite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download this just as a validation model to test the actual mic/tflite process\n",
    "if os.path.exists('./okay_nabu.tflite') == False:\n",
    "    !wget https://github.com/esphome/micro-wake-word-models/raw/main/models/okay_nabu.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q tflite_micro\n",
    "%pip install -q asciichartpy\n",
    "%pip install -q sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gradio as gr\n",
    "import tensorflow as tf\n",
    "from tflite_micro.python.tflite_micro import runtime\n",
    "import numpy as np\n",
    "import scipy.signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following swithc uses the tflite preprocessor (same one used in the embedded device) as opposed of using\n",
    "# the microfrontend s/w based one\n",
    "tflite_prep = True\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.lite.experimental.microfrontend.python.ops import audio_microfrontend_op as frontend_op\n",
    "from tflite_micro.python.tflite_micro import runtime\n",
    "\n",
    "preprocessor_model = runtime.Interpreter.from_file(\"./audio_preprocessor_int8.tflite\")\n",
    "input_details = preprocessor_model.get_input_details(0)\n",
    "output_details = preprocessor_model.get_output_details(0)\n",
    "preprocessor_model.print_allocations()\n",
    "\n",
    "class VectorSplitter:\n",
    "    def __init__(self, chunk_size=480):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.remainder = np.zeros(160)\n",
    "\n",
    "    def split_into_chunks(self, vector):\n",
    "        chunks = []\n",
    "        #print(f\"Splitting vector of size {len(vector)}, stored remainder is of size {len(self.remainder)}\")\n",
    "        vector = np.concatenate((self.remainder, vector))\n",
    "        #print(f\"Concatenated vector of size {len(vector)}\")\n",
    "        i = 0\n",
    "        while i + self.chunk_size <= len(vector):\n",
    "        #for i in range(0, len(vector), self.chunk_size):\n",
    "            chunk = vector[i:i + self.chunk_size]\n",
    "            chunks.append(chunk)\n",
    "            #print(\"append chunk \", chunk.shape, \" i:\", i)\n",
    "            #self.remainder = vector[-160:]\n",
    "            #print(\"Remainder:\", self.remainder.shape)\n",
    "            i += 320\n",
    "        #if i < len(vector):\n",
    "        self.remainder = vector[i:]\n",
    "        #print(\"end of vector Remainder:\", self.remainder.shape)\n",
    "        return chunks\n",
    "\n",
    "splitter = VectorSplitter()\n",
    "def get_features(input):\n",
    "    if len(input) != 480:\n",
    "        raise ValueError(\"Input must be of size 480\")\n",
    "        return\n",
    "    preprocessor_model.set_input(input.reshape([1,480]).astype(np.int16), 0)\n",
    "    preprocessor_model.invoke()\n",
    "    return preprocessor_model.get_output(0)\n",
    "\n",
    "# this generates the MEL spectrogram features for a given clip\n",
    "def generate_features_for_clip(clip):\n",
    "\n",
    "    if tflite_prep == True:\n",
    "        chunks =  splitter.split_into_chunks(clip)\n",
    "        matrix = np.array([get_features(chunk) for chunk in chunks])\n",
    "        return matrix\n",
    "    else:\n",
    "        micro_frontend = frontend_op.audio_microfrontend(\n",
    "            tf.convert_to_tensor(clip),\n",
    "            sample_rate=16000,\n",
    "            window_size=30,\n",
    "            window_step=20,\n",
    "            num_channels=40,\n",
    "            upper_band_limit=7500,\n",
    "            lower_band_limit=125,\n",
    "            enable_pcan=True,\n",
    "            min_signal_remaining=0.05,\n",
    "            out_scale=1,\n",
    "            out_type=tf.float32)\n",
    "        output = tf.multiply(micro_frontend, 0.0390625)\n",
    "        return output.numpy()\n",
    "\n",
    "def features_generator(generator):\n",
    "    for data in generator:\n",
    "        for clip in data:\n",
    "            yield generate_features_for_clip(clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "infer_model = tf.lite.Interpreter(model_path=\"./stream_state_internal_quant.tflite\", num_threads=1)\n",
    "#infer_model = tf.lite.Interpreter(model_path=\"./okay_nabu.tflite\", num_threads=1)\n",
    "infer_model.resize_tensor_input(0, [1,1,40], strict=True)  # initialize with fixed input size\n",
    "infer_model.allocate_tensors()\n",
    "infer_model_input_details = infer_model.get_input_details()\n",
    "infer_model_output_details = infer_model.get_output_details()\n",
    "print()\n",
    "print(\"Input details:\")\n",
    "print(infer_model_input_details)\n",
    "print()\n",
    "print(\"Output details:\")\n",
    "print(infer_model_output_details)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Mic testing WW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import asciichartpy\n",
    "from IPython.display import clear_output\n",
    "# Initialize a list to hold the last 100 data points\n",
    "last_100_data_points = [0] * 100\n",
    "\n",
    "# Define your callback here\n",
    "def process_audio_callback(indata, frames, time, status):\n",
    "    global last_100_data_points\n",
    "    indata = (indata.flatten()*32767).astype(np.int16)\n",
    "    #print(\"indata shape:\", indata.shape)\n",
    "    res = generate_features_for_clip(indata)\n",
    "    #print(\"res shape:\", res.shape)\n",
    "    # Get predictions\n",
    "    for row in res:\n",
    "        row1 = row.astype(np.int8)\n",
    "        row3 = row1.reshape([1,1,40])\n",
    "        infer_model.set_tensor(infer_model_input_details[0]['index'], row3)\n",
    "        infer_model.invoke()\n",
    "        pred = infer_model.get_tensor(infer_model_output_details[0]['index'])\n",
    "        # Update the list of last 100 data points\n",
    "        last_100_data_points = last_100_data_points[1:] + [pred[0,0]]\n",
    "        last_100_data_points[0] = 255\n",
    "\n",
    "# Set the callback to be called every 500 ms\n",
    "stream = sd.InputStream(callback=process_audio_callback, channels=1, blocksize=int(320), samplerate = 16000)\n",
    "with stream:\n",
    "    while True:\n",
    "        sd.sleep(500)\n",
    "        # Clear the console\n",
    "        clear_output(wait=True)\n",
    "        #print(last_100_data_points)\n",
    "        print(asciichartpy.plot(last_100_data_points, {\"height\": 10}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WAV file testing WW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import asciichartpy\n",
    "import itertools\n",
    "from IPython.display import clear_output\n",
    "# Initialize a list to hold the last 100 data points\n",
    "last_100_data_points = [0] * 100\n",
    "\n",
    "# Read the wav file\n",
    "data, samplerate = sf.read('./ww2.wav')\n",
    "# print(\"data shape:\", data.shape)\n",
    "# print(\"data:\", data)\n",
    "# print(\"samplerate:\", samplerate)\n",
    "# print(\"data length in sec\", data.shape[0]/16000)\n",
    "\n",
    "# Create a generator to yield samples\n",
    "def gen_samples():\n",
    "    for sample in data:\n",
    "        yield sample\n",
    "\n",
    "samples = gen_samples()\n",
    "\n",
    "# Define your callback here\n",
    "def process_audio_callback(outdata, frames, time, status):\n",
    "\n",
    "    global last_100_data_points\n",
    "\n",
    "    try:\n",
    "        outdata[:] = np.array(list(itertools.islice(samples, frames))).reshape(-1, 1)\n",
    "    except StopIteration:\n",
    "        raise sd.CallbackStop()\n",
    "    \n",
    "    indata = (outdata.flatten()*32767).astype(np.int16)\n",
    "    res = generate_features_for_clip(indata)\n",
    "\n",
    "    # Get predictions\n",
    "    for row in res:\n",
    "        row1 = row.astype(np.int8)\n",
    "        row3 = row1.reshape([1,1,40])\n",
    "        infer_model.set_tensor(infer_model_input_details[0]['index'], row3)\n",
    "        infer_model.invoke()\n",
    "        pred = infer_model.get_tensor(infer_model_output_details[0]['index'])\n",
    "        # Update the list of last 100 data points\n",
    "        last_100_data_points = last_100_data_points[1:] + [pred[0,0]]\n",
    "        # this will make sure the graph is scaled properly\n",
    "        last_100_data_points[0] = 255\n",
    "\n",
    "\n",
    "\n",
    "with sd.OutputStream(callback=process_audio_callback, blocksize=320, channels=1, samplerate=samplerate) as stream:\n",
    "    while stream.active:\n",
    "        sd.sleep(500)\n",
    "        # Clear the console\n",
    "        clear_output(wait=True)\n",
    "        #print(last_100_data_points)\n",
    "        print(asciichartpy.plot(last_100_data_points, {\"height\": 10}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
