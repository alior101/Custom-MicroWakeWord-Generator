{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC7e0bC587b6"
   },
   "source": [
    "The notebook load the dataset from the Google drive and does inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab\n"
     ]
    }
   ],
   "source": [
    "# Install all the required packages (borrowed from openWakeWord's automatic training notebook)\n",
    "running_on_colab = False\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Running on CoLab')\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    running_on_colab = True\n",
    "    restore_dataset = True\n",
    "    restore_features = True\n",
    "else:\n",
    "    print('Not running on CoLab')\n",
    "    restore_dataset = False\n",
    "    restore_features = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if running_on_colab:\n",
    "    # restoring previous trained model for finetuning from previously generated features\n",
    "    trained_model_filename = \"/content/drive/MyDrive/ColabNotebooks/VoiceAssistant/microWakeWord/trained_models_20240421_164022.tar\"\n",
    "    !cp {trained_model_filename}  .\n",
    "\n",
    "    model_filename = os.path.basename(trained_model_filename)\n",
    "    !tar -xvf {model_filename}\n",
    "\n",
    "if os.path.exists('audio_preprocessor_int8.tflite') == False:\n",
    "    !wget https://github.com/tensorflow/tflite-micro/raw/main/tensorflow/lite/micro/examples/micro_speech/models/audio_preprocessor_int8.tflite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-29 17:02:07--  https://github.com/esphome/micro-wake-word-models/raw/main/models/okay_nabu.tflite\n",
      "Resolving github.com (github.com)... 140.82.114.3\n",
      "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/esphome/micro-wake-word-models/main/models/okay_nabu.tflite [following]\n",
      "--2024-04-29 17:02:07--  https://raw.githubusercontent.com/esphome/micro-wake-word-models/main/models/okay_nabu.tflite\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 115400 (113K) [application/octet-stream]\n",
      "Saving to: 'okay_nabu.tflite'\n",
      "\n",
      "okay_nabu.tflite    100%[===================>] 112.70K  --.-KB/s    in 0.08s   \n",
      "\n",
      "2024-04-29 17:02:08 (1.39 MB/s) - 'okay_nabu.tflite' saved [115400/115400]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download this just as a validation model to test the actual mic/tflite process\n",
    "if os.path.exists('./okay_nabu.tflite') == False:\n",
    "    !wget https://github.com/esphome/micro-wake-word-models/raw/main/models/okay_nabu.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q tflite_micro\n",
    "%pip install -q asciichartpy\n",
    "%pip install -q sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 19:47:32.949697: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-29 19:47:32.949742: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-29 19:47:32.950855: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-29 19:47:32.994386: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#import gradio as gr\n",
    "import tensorflow as tf\n",
    "from tflite_micro.python.tflite_micro import runtime\n",
    "import numpy as np\n",
    "import scipy.signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of resource variables the model uses =  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RecordingMicroAllocator] Arena allocation total 11800 bytes\n",
      "[RecordingMicroAllocator] Arena allocation head 3040 bytes\n",
      "[RecordingMicroAllocator] Arena allocation tail 8760 bytes\n",
      "[RecordingMicroAllocator] 'TfLiteEvalTensor data' used 1032 bytes with alignment overhead (requested 1032 bytes for 43 allocations)\n",
      "[RecordingMicroAllocator] 'Persistent TfLiteTensor data' used 224 bytes with alignment overhead (requested 224 bytes for 2 tensors)\n",
      "[RecordingMicroAllocator] 'Persistent buffer data' used 4988 bytes with alignment overhead (requested 4872 bytes for 19 allocations)\n",
      "[RecordingMicroAllocator] 'NodeAndRegistration struct' used 1936 bytes with alignment overhead (requested 1936 bytes for 22 NodeAndRegistration structs)\n"
     ]
    }
   ],
   "source": [
    "# the following swithc uses the tflite preprocessor (same one used in the embedded device) as opposed of using\n",
    "# the microfrontend s/w based one\n",
    "tflite_prep = True\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.lite.experimental.microfrontend.python.ops import audio_microfrontend_op as frontend_op\n",
    "from tflite_micro.python.tflite_micro import runtime\n",
    "\n",
    "preprocessor_model = runtime.Interpreter.from_file(\"./audio_preprocessor_int8.tflite\")\n",
    "input_details = preprocessor_model.get_input_details(0)\n",
    "output_details = preprocessor_model.get_output_details(0)\n",
    "preprocessor_model.print_allocations()\n",
    "\n",
    "class VectorSplitter:\n",
    "    def __init__(self, chunk_size=480):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.remainder = np.zeros(160)\n",
    "\n",
    "    def split_into_chunks(self, vector):\n",
    "        chunks = []\n",
    "        #print(f\"Splitting vector of size {len(vector)}, stored remainder is of size {len(self.remainder)}\")\n",
    "        vector = np.concatenate((self.remainder, vector))\n",
    "        #print(f\"Concatenated vector of size {len(vector)}\")\n",
    "        i = 0\n",
    "        while i + self.chunk_size <= len(vector):\n",
    "        #for i in range(0, len(vector), self.chunk_size):\n",
    "            chunk = vector[i:i + self.chunk_size]\n",
    "            chunks.append(chunk)\n",
    "            #print(\"append chunk \", chunk.shape, \" i:\", i)\n",
    "            #self.remainder = vector[-160:]\n",
    "            #print(\"Remainder:\", self.remainder.shape)\n",
    "            i += 320\n",
    "        #if i < len(vector):\n",
    "        self.remainder = vector[i:]\n",
    "        #print(\"end of vector Remainder:\", self.remainder.shape)\n",
    "        return chunks\n",
    "\n",
    "splitter = VectorSplitter()\n",
    "def get_features(input):\n",
    "    if len(input) != 480:\n",
    "        raise ValueError(\"Input must be of size 480\")\n",
    "        return\n",
    "    preprocessor_model.set_input(input.reshape([1,480]).astype(np.int16), 0)\n",
    "    preprocessor_model.invoke()\n",
    "    return preprocessor_model.get_output(0)\n",
    "\n",
    "# this generates the MEL spectrogram features for a given clip\n",
    "def generate_features_for_clip(clip):\n",
    "\n",
    "    if tflite_prep == True:\n",
    "        chunks =  splitter.split_into_chunks(clip)\n",
    "        matrix = np.array([get_features(chunk) for chunk in chunks])\n",
    "        return matrix\n",
    "    else:\n",
    "        micro_frontend = frontend_op.audio_microfrontend(\n",
    "            tf.convert_to_tensor(clip),\n",
    "            sample_rate=16000,\n",
    "            window_size=30,\n",
    "            window_step=20,\n",
    "            num_channels=40,\n",
    "            upper_band_limit=7500,\n",
    "            lower_band_limit=125,\n",
    "            enable_pcan=True,\n",
    "            min_signal_remaining=0.05,\n",
    "            out_scale=1,\n",
    "            out_type=tf.float32)\n",
    "        output = tf.multiply(micro_frontend, 0.0390625)\n",
    "        return output.numpy()\n",
    "\n",
    "def features_generator(generator):\n",
    "    for data in generator:\n",
    "        for clip in data:\n",
    "            yield generate_features_for_clip(clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input details:\n",
      "[{'name': 'serving_default_input_audio:0', 'index': 0, 'shape': array([ 1,  1, 40], dtype=int32), 'shape_signature': array([ 1,  1, 40], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.9803921580314636, 3), 'quantization_parameters': {'scales': array([0.98039216], dtype=float32), 'zero_points': array([3], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "\n",
      "Output details:\n",
      "[{'name': 'StatefulPartitionedCall:0', 'index': 129, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([1, 1], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.00390625, 0), 'quantization_parameters': {'scales': array([0.00390625], dtype=float32), 'zero_points': array([0], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "infer_model = tf.lite.Interpreter(model_path=\"./trained_models/alexha/tflite_stream_state_internal_quant/stream_state_internal_quant.tflite\", num_threads=1)\n",
    "#infer_model = tf.lite.Interpreter(model_path=\"./okay_nabu.tflite\", num_threads=1)\n",
    "infer_model.resize_tensor_input(0, [1,1,40], strict=True)  # initialize with fixed input size\n",
    "infer_model.allocate_tensors()\n",
    "infer_model_input_details = infer_model.get_input_details()\n",
    "infer_model_output_details = infer_model.get_output_details()\n",
    "print()\n",
    "print(\"Input details:\")\n",
    "print(infer_model_input_details)\n",
    "print()\n",
    "print(\"Output details:\")\n",
    "print(infer_model_output_details)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "PortAudioError",
     "evalue": "Error querying device -1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPortAudioError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m         last_100_data_points[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Set the callback to be called every 500 ms\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[43msd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInputStream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess_audio_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m320\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m stream:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/google_kws/lib/python3.11/site-packages/sounddevice.py:1421\u001b[0m, in \u001b[0;36mInputStream.__init__\u001b[0;34m(self, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, samplerate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, blocksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1392\u001b[0m              device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, latency\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1393\u001b[0m              extra_settings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, finished_callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1394\u001b[0m              clip_off\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither_off\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, never_drop_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1395\u001b[0m              prime_output_buffers_using_stream_callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"PortAudio input stream (using NumPy).\u001b[39;00m\n\u001b[1;32m   1397\u001b[0m \n\u001b[1;32m   1398\u001b[0m \u001b[38;5;124;03m    This has the same methods and attributes as `Stream`, except\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \n\u001b[1;32m   1420\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1421\u001b[0m     \u001b[43m_StreamBase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrap_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marray\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[43m                         \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_remove_self\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/google_kws/lib/python3.11/site-packages/sounddevice.py:817\u001b[0m, in \u001b[0;36m_StreamBase.__init__\u001b[0;34m(self, kind, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback, userdata, wrap_callback)\u001b[0m\n\u001b[1;32m    814\u001b[0m         samplerate \u001b[38;5;241m=\u001b[39m isamplerate\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    816\u001b[0m     parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_samplesize, samplerate \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m--> 817\u001b[0m         \u001b[43m_get_stream_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mextra_settings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device \u001b[38;5;241m=\u001b[39m parameters\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels \u001b[38;5;241m=\u001b[39m parameters\u001b[38;5;241m.\u001b[39mchannelCount\n",
      "File \u001b[0;32m/opt/conda/envs/google_kws/lib/python3.11/site-packages/sounddevice.py:2660\u001b[0m, in \u001b[0;36m_get_stream_parameters\u001b[0;34m(kind, device, channels, dtype, latency, extra_settings, samplerate)\u001b[0m\n\u001b[1;32m   2657\u001b[0m     samplerate \u001b[38;5;241m=\u001b[39m default\u001b[38;5;241m.\u001b[39msamplerate\n\u001b[1;32m   2659\u001b[0m device \u001b[38;5;241m=\u001b[39m _get_device_id(device, kind, raise_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2660\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[43mquery_devices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m channels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2662\u001b[0m     channels \u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m kind \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_channels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/google_kws/lib/python3.11/site-packages/sounddevice.py:569\u001b[0m, in \u001b[0;36mquery_devices\u001b[0;34m(device, kind)\u001b[0m\n\u001b[1;32m    567\u001b[0m info \u001b[38;5;241m=\u001b[39m _lib\u001b[38;5;241m.\u001b[39mPa_GetDeviceInfo(device)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m info:\n\u001b[0;32m--> 569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PortAudioError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError querying device \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m info\u001b[38;5;241m.\u001b[39mstructVersion \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    571\u001b[0m name_bytes \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mstring(info\u001b[38;5;241m.\u001b[39mname)\n",
      "\u001b[0;31mPortAudioError\u001b[0m: Error querying device -1"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import asciichartpy\n",
    "from IPython.display import clear_output\n",
    "# Initialize a list to hold the last 100 data points\n",
    "last_100_data_points = [0] * 100\n",
    "\n",
    "# Define your callback here\n",
    "def process_audio_callback(indata, frames, time, status):\n",
    "    global last_100_data_points\n",
    "    indata = (indata.flatten()*32767).astype(np.int16)\n",
    "    #print(\"indata shape:\", indata.shape)\n",
    "    res = generate_features_for_clip(indata)\n",
    "    #print(\"res shape:\", res.shape)\n",
    "    # Get predictions\n",
    "    for row in res:\n",
    "        row1 = row.astype(np.int8)\n",
    "        row3 = row1.reshape([1,1,40])\n",
    "        infer_model.set_tensor(infer_model_input_details[0]['index'], row3)\n",
    "        infer_model.invoke()\n",
    "        pred = infer_model.get_tensor(infer_model_output_details[0]['index'])\n",
    "        # Update the list of last 100 data points\n",
    "        last_100_data_points = last_100_data_points[1:] + [pred[0,0]]\n",
    "        last_100_data_points[0] = 255\n",
    "\n",
    "# Set the callback to be called every 500 ms\n",
    "stream = sd.InputStream(callback=process_audio_callback, channels=1, blocksize=int(320), samplerate = 16000)\n",
    "with stream:\n",
    "    while True:\n",
    "        sd.sleep(500)\n",
    "        # Clear the console\n",
    "        clear_output(wait=True)\n",
    "        #print(last_100_data_points)\n",
    "        print(asciichartpy.plot(last_100_data_points, {\"height\": 10}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txh26uwwsmez"
   },
   "source": [
    "Backup model for later continued training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
